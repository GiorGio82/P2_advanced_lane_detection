{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Lane Finding Project\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "\n",
    "* Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.\n",
    "* Apply a distortion correction to raw images.\n",
    "* Use color transforms, gradients, etc., to create a thresholded binary image.\n",
    "* Apply a perspective transform to rectify binary image (\"birds-eye view\").\n",
    "* Detect lane pixels and fit to find the lane boundary.\n",
    "* Determine the curvature of the lane and vehicle position with respect to center.\n",
    "* Warp the detected lane boundaries back onto the original image.\n",
    "* Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.\n",
    "\n",
    "---\n",
    "## Please run all cells until the cell called \"Processing project video\"\n",
    "unfortunately the current pipeline does not work neither with the challenge nor the harder challenge videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries and pickle file\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n",
    "import collections\n",
    "from IPython.display import HTML\n",
    "import scipy.spatial\n",
    "%matplotlib qt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undistort(img, mtx, dist):\n",
    "    '''\n",
    "    method: undistort input image\n",
    "    input: \n",
    "        image to undistort, \n",
    "        camera matrix\n",
    "        distortion coeffs\n",
    "    output: undistorted image \n",
    "    '''\n",
    "    undist = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    return undist  \n",
    "\n",
    "def binary_sobelx(color_img,kernel_size=3,sx_thresh=(20, 100), gauss_blur=False, gauss_k_size = 3):\n",
    "    '''\n",
    "    method: helper method that applies the Sobel operator on the x direction \n",
    "    input: \n",
    "        original image (in colored format), \n",
    "        sobel kernel size, \n",
    "        thresholds for the sobel operator (range 0-255)\n",
    "        flag for applying the gaussian blur\n",
    "        gaussian blur kernel size (used only if flag is true)\n",
    "    output:\n",
    "        binary thresholded copy of the input image with applied sobel operator on the x axis\n",
    "    '''\n",
    "    # 1) Convert to grayscale\n",
    "    gray = cv2.cvtColor(np.copy(color_img), cv2.COLOR_RGB2GRAY)  \n",
    "    \n",
    "    # 2) Take both Sobel x and apply gaussian blur\n",
    "    if gauss_blur:\n",
    "        smoothed = gaussian_blur(gray,gauss_k_size)\n",
    "        sobelx = cv2.Sobel(smoothed, cv2.CV_64F, 1, 0,ksize=kernel_size) # Take the derivative in x\n",
    "    else:\n",
    "        sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0,ksize=kernel_size) # Take the derivative in x\n",
    "\n",
    "    # 3)Absolute x derivative to accentuate lines away from horizontal\n",
    "    abs_sobelx = np.absolute(sobelx) \n",
    "    scaled_sobel = np.uint8(255*abs_sobelx/np.max(abs_sobelx))\n",
    "    \n",
    "    # 4)Threshold x gradient, apply mask\n",
    "    sxbinary = np.zeros_like(scaled_sobel)\n",
    "    sxbinary[(scaled_sobel >= sx_thresh[0]) & (scaled_sobel <= sx_thresh[1])] = 1\n",
    "    \n",
    "    return sxbinary\n",
    "\n",
    "def sobel_mag_thresh(color_img, kernel_size=3, mag_thresh=(30, 100)):\n",
    "    '''\n",
    "    method: helper method that calculates the Sobel gradient \n",
    "    magnitue (on both x and y axes), applies to the input \n",
    "    image and returns a binary image \n",
    "    input: \n",
    "        original image (in colored format), \n",
    "        sobel kernel size, \n",
    "        thresholds for the gradient magniture (range 0-255)\n",
    "    output:\n",
    "        binary thresholded copy of the input image with applied sobel operator on both x and y axes\n",
    "    ''' \n",
    "    # 1) Convert to grayscale\n",
    "    gray = cv2.cvtColor(np.copy(color_img), cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # 2) Take both Sobel x and y gradients\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=kernel_size)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=kernel_size)\n",
    "    \n",
    "    # 3) Calculate the magnitude\n",
    "    mag = np.sqrt(sobelx**2 + sobely**2)\n",
    "    \n",
    "    # 4) Scale to 8-bit (0 - 255) and convert to type = np.uint8\n",
    "    scale_factor = np.max(mag)/255 \n",
    "    norm_mag = (mag/scale_factor).astype(np.uint8)     \n",
    "    \n",
    "    # 5) Create a binary mask where mag thresholds are met\n",
    "    binary_mag = np.zeros_like(norm_mag)\n",
    "    binary_mag[(norm_mag >= mag_thresh[0]) & (norm_mag <=mag_thresh[1])] = 1\n",
    "    \n",
    "    return binary_mag\n",
    "\n",
    "def binary_s_channel(color_img,s_thresh=(150, 255)):\n",
    "    '''\n",
    "    method: helper method that gets the saturation channel \n",
    "    of an image and generates a binary copy of it using \n",
    "    some thresholds in the HLS color space\n",
    "    input: \n",
    "        original image (in colored format),  \n",
    "        thresholds for the saturation channel (range 0-255)\n",
    "    output:\n",
    "        binary thresholded copy of the input image with applied a mask on the saturation channel\n",
    "    '''   \n",
    "    #1)make a copy of the image and transform it into HLS space \n",
    "    hls = cv2.cvtColor(np.copy(color_img), cv2.COLOR_RGB2HLS)\n",
    "    #2)get the saturation channel\n",
    "    s_channel = hls[:,:,2]\n",
    "    #3)apply masks\n",
    "    s_binary = np.zeros_like(s_channel)\n",
    "    s_binary[(s_channel >= s_thresh[0]) & (s_channel <= s_thresh[1])] = 1    \n",
    "    \n",
    "    return s_binary\n",
    "\n",
    "def binary_r_channel(color_img,r_thresh=(200,255)):\n",
    "    '''\n",
    "    method: helper method that gets the red channel \n",
    "    of an image and generates a binary copy of it using \n",
    "    some thresholds in the RGB color space\n",
    "    input: \n",
    "        original image (in colored format),  \n",
    "        thresholds for the red channel (range 0-255)\n",
    "    output:\n",
    "        binary thresholded copy of the input image with applied a mask on the red channel\n",
    "    ''' \n",
    "    #1)copy the image\n",
    "    img = np.copy(color_img)\n",
    "    #2) get red channel\n",
    "    r_channel = img[:,:,0]\n",
    "    #3)apply masks\n",
    "    r_binary = np.zeros_like(r_channel)\n",
    "    r_binary[(r_channel >= r_thresh[0]) & (r_channel <= r_thresh[1])] = 1      \n",
    "    \n",
    "    return r_binary \n",
    "\n",
    "def binary_filters(s_x = True, bin_sx=0,s_mag=True, bin_s_mag=0, s_ch=True,bin_s_ch=0,r_ch=True,bin_r_ch=0):\n",
    "    '''\n",
    "    method: helper method that applies logical OR to all input images  \n",
    "    input: \n",
    "        s_x sobel x flag\n",
    "        bin_sx sobel x binary image\n",
    "        s_mag gradient magnitude flag\n",
    "        bin_s_mag gradient masked binary image\n",
    "        s_ch saturation channel flag\n",
    "        bin_s_ch binary-masked-saturation image\n",
    "        r_ch red channel flag\n",
    "        bin_r_ch binary-masked-red image\n",
    "    output:\n",
    "        binary thresholded copy that is the logical OR of the inputs\n",
    "    '''     \n",
    "    #init\n",
    "    if s_x == True:\n",
    "        binary_mask = np.zeros_like(bin_sx)\n",
    "    elif s_mag == True:\n",
    "        binary_mask = np.zeros_like(bin_s_mag)\n",
    "    elif s_ch == True:\n",
    "        binary_mask = np.zeros_like(bin_s_ch)\n",
    "    elif r_ch == True:\n",
    "        binary_mask = np.zeros_like(bin_r_ch)\n",
    "\n",
    "    #apply only specific filters\n",
    "    if (s_x == True) :\n",
    "        binary_mask[(bin_sx ==1) | (binary_mask ==1)] = 1                \n",
    "    if (s_mag == True):\n",
    "        binary_mask[(bin_s_mag ==1) | (binary_mask ==1)] = 1     \n",
    "    if (s_ch == True):\n",
    "        binary_mask[(bin_s_ch ==1) | (binary_mask ==1)] = 1\n",
    "    if (r_ch == True):\n",
    "        binary_mask[(bin_r_ch ==1) | (binary_mask ==1)] = 1 \n",
    " \n",
    "    return binary_mask\n",
    "\n",
    "def colors_and_gradients(input_image):\n",
    "    '''\n",
    "    method: apply color filters and gradient operators to the input image.    \n",
    "    input: \n",
    "        input_image\n",
    "    output:\n",
    "        binary_or_img image that is the logical OR of different masks\n",
    "        bin_sobelx binary image with applied sobel operator in the x direction\n",
    "        bin_s_channel binary image with applied a threshold on the saturation channel\n",
    "        bin_r_channel binary image with applied a threshold on the red channel\n",
    "        bin_sobel_mag binary image with applied sobel gradient magnitude thresholds\n",
    "    '''      \n",
    "    #apply all filters to the input image\n",
    "    #input values are the best values found by observing different combinations\n",
    "    bin_sobelx = binary_sobelx(input_image,kernel_size=5,sx_thresh=(20, 100),gauss_blur=True)\n",
    "    bin_s_channel = binary_s_channel(input_image,s_thresh=(150, 255))\n",
    "    bin_r_channel =  binary_r_channel(input_image,r_thresh=(200,255))\n",
    "    bin_sobel_mag = sobel_mag_thresh(input_image, kernel_size=5, mag_thresh=(30, 100)) \n",
    "\n",
    "    #check how many active pixels there are in the red channel and apply a filter.\n",
    "    #the red channel is very sensitive at times, therefore I filter it out \n",
    "    #in some cases\n",
    "    count_light_in_red_ch =  np.sum(bin_r_channel[:,:])\n",
    "    \n",
    "    #the threshold is the result of some visual observations\n",
    "    #if there are too many activatd pixels in the red channel I exlude it\n",
    "    if count_light_in_red_ch > 150000:\n",
    "        binary_or_img = binary_filters(s_x = True, bin_sx = bin_sobelx,\n",
    "                                   s_mag=True, bin_s_mag = bin_sobel_mag, \n",
    "                                   s_ch=True,bin_s_ch=bin_s_channel,\n",
    "                                   r_ch=False,bin_r_ch=bin_r_channel)\n",
    "    else:\n",
    "        binary_or_img = binary_filters(s_x = True, bin_sx = bin_sobelx,\n",
    "                                   s_mag=True, bin_s_mag = bin_sobel_mag, \n",
    "                                   s_ch=True,bin_s_ch=bin_s_channel,\n",
    "                                   r_ch=True,bin_r_ch=bin_r_channel)\n",
    "    \n",
    "    return binary_or_img,bin_sobelx,bin_s_channel,bin_r_channel,bin_sobel_mag\n",
    "\n",
    "def gaussian_blur(img, kernel_size):\n",
    "    '''\n",
    "    method: Applies a Gaussian Noise kernel    \n",
    "    input: \n",
    "        input_image\n",
    "        kernel size of the operator\n",
    "    output:\n",
    "        image with gaussian blur applied\n",
    "    '''  \n",
    "    blurred = cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)\n",
    "    return blurred\n",
    "\n",
    "\n",
    "\n",
    "def chessboard_warper(img, nx, ny, mtx, dist):\n",
    "    '''\n",
    "    method: Applies a perspective transformation to a warped chessboard \n",
    "    image. The edge corners of the final image have an offset from the image edges   \n",
    "    input: \n",
    "        img input_image\n",
    "        nx number of columns of the chessboard  \n",
    "        ny number of rows of the chessboard\n",
    "        mtx camera matrix\n",
    "        dist camera distortion coeffs\n",
    "    output:\n",
    "        unwarped chesshboard (if corners are found in the input image), return 0,0 instead\n",
    "    '''\n",
    "    #fix offset\n",
    "    offset = 100\n",
    "    \n",
    "    #image size\n",
    "    img_size = (img.shape[1], img.shape[0])\n",
    "    \n",
    "    #if some corners are found, use the edge corners as reference points\n",
    "    #for the warp perspective\n",
    "    ret, corners = cv2.findChessboardCorners(img, (nx,ny), None)\n",
    "    if ret == True: \n",
    "\n",
    "        src = np.float32([corners[0], corners[nx-1], corners[-1], corners[-nx]])\n",
    "\n",
    "        dst = np.float32([[offset, offset], [img_size[0]-offset, offset], \n",
    "                          [img_size[0]-offset, img_size[1]-offset], [offset, img_size[1]-offset]])\n",
    "        \n",
    "        #warped, M, invM = warper(img, src, dst)\n",
    "        M = cv2.getPerspectiveTransform(src, dst)\n",
    "        warped = cv2.warpPerspective(img,M,img_size,flags=cv2.INTER_LINEAR)\n",
    "        \n",
    "        return warped\n",
    "    else:\n",
    "        return 0,0\n",
    "\n",
    "def warper(img, src, dst):\n",
    "    '''\n",
    "    method: Applies a warp perspective transformation to any input image        \n",
    "    input: \n",
    "        img input image \n",
    "        src coordinate of the warp reference points\n",
    "        dst coordinates of the warp destination points\n",
    "    output:\n",
    "        warped image\n",
    "        M transformation matrix used to warp the image\n",
    "        invM inverse transformation matrix (to unwarp the image)\n",
    "    '''\n",
    "    # Compute and apply perpective transform\n",
    "    img_size = (img.shape[1], img.shape[0])\n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "    #warped = cv2.warpPerspective(img, M, img_size, flags=cv2.INTER_NEAREST)  # keep same size as input image\n",
    "    warped = cv2.warpPerspective(img,M,img_size,flags=cv2.INTER_LINEAR)\n",
    "    invM = cv2.getPerspectiveTransform(dst,src)\n",
    "    \n",
    "    return warped, M, invM\n",
    "      \n",
    "def getWarpSrcPts(img):\n",
    "    '''\n",
    "    method: get the coordinates of the source perspective transform points \n",
    "    from the give image. The hardcoded offset points are the result of several manual test.\n",
    "    I decided to remove at least almost the entire car hood too. Please note that the order of \n",
    "    the output array is P3,P4,P1,P2\n",
    "    input: \n",
    "        img input image \n",
    "    output:\n",
    "        coordinates of 4 source points\n",
    "    '''\n",
    "    x_offset_up = 550\n",
    "    x_offset_low = 170\n",
    "    y_offset_up = 250\n",
    "    y_car_hood = 30\n",
    "    \n",
    "    #lower left corner of Trapezoid\n",
    "    P1 = (x_offset_low,img.shape[0]-y_car_hood)\n",
    "    #upper left corner of Trapezoid\n",
    "    P2 = (x_offset_up, img.shape[0]-y_offset_up) \n",
    "    #upper right corner of Trapezoid\n",
    "    P3 = (int(round(img.shape[1]-x_offset_up)), img.shape[0]-y_offset_up)\n",
    "    #lower right corner of Trapezoid\n",
    "    P4 = (img.shape[1]-x_offset_low,img.shape[0]-y_car_hood)\n",
    "    \n",
    "    return  np.float32([P3,P4,P1,P2]) \n",
    "\n",
    "def getWarpDstPts(img):\n",
    "    '''\n",
    "    method: get the coordinates of the destination perspective transform points \n",
    "    from the give image. The hardcoded offset points are the result of several \n",
    "    manual tests. Please note that the order of the output array is P3,P4,P1,P2\n",
    "    input: \n",
    "        img input image \n",
    "    output:\n",
    "        coordinates of 4 destination points\n",
    "    '''\n",
    "    x_offset = 300\n",
    "    y_offset_up = 50\n",
    "\n",
    "\n",
    "    #lower left corner of Trapezoid\n",
    "    P1 = (x_offset,img.shape[0])\n",
    "    #upper left corner of Trapezoid\n",
    "    P2 = (x_offset, y_offset_up) \n",
    "    #upper right corner of Trapezoid\n",
    "    P3 = (img.shape[1]-x_offset, y_offset_up)\n",
    "    #lower right corner of Trapezoid\n",
    "    P4 = (img.shape[1]-x_offset,img.shape[0])    \n",
    "    \n",
    "    \n",
    "    return  np.float32([P3,P4,P1,P2])\n",
    "\n",
    "def getWarpSrcPtsChallenge(img):\n",
    "    '''\n",
    "    method: get the coordinates of the source perspective transform points \n",
    "    from the give image. The hardcoded offset points are the result of several manual test.\n",
    "    I decided to remove at least almost the entire car hood too. Please note that the order of \n",
    "    the output array is P3,P4,P1,P2\n",
    "    input: \n",
    "        img input image \n",
    "    output:\n",
    "        coordinates of 4 source points\n",
    "    '''\n",
    "    x_offset_up_left = 550\n",
    "    x_offset_up_right = 520\n",
    "    x_offset_low = 170\n",
    "    y_offset_up = 230\n",
    "    y_car_hood = 30\n",
    "    \n",
    "    #lower left corner of Trapezoid\n",
    "    P1 = (x_offset_low,img.shape[0]-y_car_hood)\n",
    "    #upper left corner of Trapezoid\n",
    "    P2 = (x_offset_up_left, img.shape[0]-y_offset_up) \n",
    "    #upper right corner of Trapezoid\n",
    "    P3 = (img.shape[1]-x_offset_up_right, img.shape[0]-y_offset_up)\n",
    "    #lower right corner of Trapezoid\n",
    "    P4 = (img.shape[1]-x_offset_low,img.shape[0]-y_car_hood)\n",
    "    \n",
    "    return  np.float32([P3,P4,P1,P2]) \n",
    "      \n",
    "    \n",
    "    return  np.float32([P3,P4,P1,P2])\n",
    "def undistort_and_warp(img,challenge=False):\n",
    "    '''\n",
    "    method: applies both undistortion and warp to the input image\n",
    "    input: \n",
    "        img input image \n",
    "    output:\n",
    "        warped image\n",
    "        M transformation matrix used to warp the image\n",
    "        invM inverse transformation matrix (to unwarp the image)\n",
    "        src = source points for the perspective transform\n",
    "        dst = destination points for the perspective transform \n",
    "    '''    \n",
    "    undist_img = undistort(img, mtx, dist)\n",
    "    \n",
    "    if (challenge == False):\n",
    "        src = getWarpSrcPts(undist_img)\n",
    "    else:\n",
    "        src = getWarpSrcPtsChallenge(undist_img)\n",
    "    dst = getWarpDstPts(undist_img)\n",
    "\n",
    "    warped,M,invM = warper(undist_img, src, dst)    \n",
    "    \n",
    "\n",
    "    return warped,M,invM,src,dst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the calibration process\n",
    "the calibration process produces coefficients used later in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "camera calibration done\n"
     ]
    }
   ],
   "source": [
    "def populate_points(images,objpt,nx,ny):\n",
    "    '''\n",
    "    method: generate a list of image points and object points for calibration\n",
    "    input: \n",
    "        images = series of chessboard images\n",
    "        objpt = coordinates to generate the objpoints\n",
    "        nx = number of columns in the chessboard\n",
    "        ny = number of rows in the chessboard\n",
    "    output:\n",
    "        objpoints = 3d points in real world space\n",
    "        imgpoints = 2d points in image plane\n",
    "    '''     \n",
    "    # Arrays to store object points and image points from all the images.\n",
    "    objpoints = [] # 3d points in real world space\n",
    "    imgpoints = [] # 2d points in image plane. \n",
    "    \n",
    "    for im in images:\n",
    "        img = cv2.imread(im)\n",
    "        imsize = img.shape[1::-1]\n",
    "        #convert to gray\n",
    "        gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        ret, corners = cv2.findChessboardCorners(gray,(nx,ny),None)\n",
    "\n",
    "        if ret == True:\n",
    "            objpoints.append(objpt) #always append the same points\n",
    "            imgpoints.append(corners) #append new corners\n",
    "\n",
    "    return objpoints,imgpoints\n",
    "\n",
    "def calibrate(imsize, objpoints, imgpoints):\n",
    "    '''\n",
    "    method: calibrate the camera\n",
    "    input: \n",
    "        imsize = size of the image\n",
    "        objpoints = 3d points in real world space\n",
    "        imgpoints = 2d points in image plane\n",
    "    output:\n",
    "        ret = boolean flag, a value was returned\n",
    "        mtx = camera matrix\n",
    "        dist = distortion coefficients\n",
    "        rvecs = rotation vectors\n",
    "        tvecs = translation vectors\n",
    "    '''  \n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints,imgpoints,imsize,None,None)\n",
    "    \n",
    "    return ret, mtx, dist, rvecs, tvecs\n",
    "\n",
    "\n",
    "def run_calibration_process():\n",
    "    '''\n",
    "    method: run the camera calibration process using a set of chessboard images\n",
    "    input: \n",
    "        none\n",
    "    output:\n",
    "        mtx = camera matrix\n",
    "        dist = distortion coefficients\n",
    "        rvecs = rotation vectors\n",
    "        tvecs = translation vectors\n",
    "    '''      \n",
    "    # Make a list of calibration images\n",
    "    images = glob.glob('camera_cal/calibration*.jpg')    \n",
    "\n",
    "    #assuming all images have the same size\n",
    "    img = cv2.imread(images[0])\n",
    "    imsize = img.shape[1::-1]\n",
    "\n",
    "    nx = 9\n",
    "    ny = 6\n",
    "\n",
    "    #build a grid of 3D coordinates\n",
    "    #x,y coordinates, z remains zero\n",
    "    objpt = np.zeros((nx*ny,3),np.float32)\n",
    "    \n",
    "    #mgrid returns the coordinates values for a given gridsize\n",
    "    #then we shape the coordinates back in two colums one for x and one for y\n",
    "    objpt[:,:2] = np.mgrid[0:nx,0:ny].T.reshape(-1,2) \n",
    "\n",
    "    #populate obj and img points \n",
    "    objpoints,imgpoints = populate_points(images,objpt,nx,ny)\n",
    "    #calibrate the camera\n",
    "    ret, mtx, dist, rvecs, tvecs = calibrate(imsize, objpoints, imgpoints)\n",
    "    print(\"camera calibration done\")\n",
    "    return mtx, dist, rvecs, tvecs\n",
    "\n",
    "mtx, dist, rvecs, tvecs  = run_calibration_process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing: undistort and warp chessboard images as test\n",
    "please uncomment the last line to visualize the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeup_plot_undistort_warp_chessboard():\n",
    "    '''\n",
    "    method: generates one image for the project writeup showing the unwarp process of two \n",
    "    chessboard images from the set of the calibration images        \n",
    "    '''  \n",
    "    \n",
    "    #open the first image\n",
    "    filename = 'camera_cal/calibration3.jpg'\n",
    "    img1 = mpimg.imread(filename)\n",
    "\n",
    "    imNum1 = filename.split('camera_cal/calibration')[1].split('.jpg')[0]\n",
    "\n",
    "    nx = 9\n",
    "    ny = 6\n",
    "\n",
    "    undistorted1= undistort(img1, mtx, dist)\n",
    "    gray1 = cv2.cvtColor(undistorted1, cv2.COLOR_RGB2GRAY)\n",
    "    top_down1 = chessboard_warper(gray1, nx, ny, mtx, dist)\n",
    "\n",
    "    #open a second image with different nx and ny\n",
    "    filename = 'camera_cal/calibration1.jpg'\n",
    "    img2 = mpimg.imread(filename)\n",
    "\n",
    "    imNum2 = filename.split('camera_cal/calibration')[1].split('.jpg')[0]\n",
    "\n",
    "    nx = 9\n",
    "    ny = 5\n",
    "\n",
    "    undistorted2 = undistort(img2, mtx, dist)\n",
    "    gray2 = cv2.cvtColor(undistorted2, cv2.COLOR_RGB2GRAY)\n",
    "    top_down2 = chessboard_warper(gray2, nx, ny, mtx, dist)\n",
    "\n",
    "\n",
    "    f, ((ax1, ax2), (ax3,ax4),(ax5,ax6)) = plt.subplots(3,2, figsize=(24, 9))\n",
    "    f.tight_layout()\n",
    "    ax1.imshow(img1)\n",
    "    ax1.set_title('Original calibration image {}'.format(imNum1), fontsize=10)\n",
    "    ax3.imshow(undistorted1)\n",
    "    ax3.set_title('Undistorted image {}'.format(imNum1), fontsize=10)\n",
    "    ax5.imshow(top_down1, cmap='gray')\n",
    "    ax5.set_title('Undistorted and Warped  image {}'.format(imNum1), fontsize=10)\n",
    "    ax2.imshow(img2)\n",
    "    ax2.set_title('Original calibration image {}'.format(imNum2), fontsize=10)\n",
    "    ax4.imshow(undistorted2)\n",
    "    ax4.set_title('Undistorted image {}'.format(imNum2), fontsize=10)\n",
    "    ax6.imshow(top_down2,cmap ='gray')\n",
    "    ax6.set_title('Undistorted and Warped image {}'.format(imNum2), fontsize=10)\n",
    "\n",
    "\n",
    "    f.savefig('output_images/undistort_warp_chessboard_examples.jpg')    \n",
    "#writeup_plot_undistort_warp_chessboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing: undistort road images with straight lines for validation\n",
    "please uncomment the last line to visualize the image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeup_plot_undistort_straight_lines():\n",
    "    '''\n",
    "    method: generates one image for the project writeup showing the \n",
    "    undistort result applied to the two test images with straight lines         \n",
    "    '''      \n",
    "    #undistorting straight line images\n",
    "    test_img_1 = 'test_images/straight_lines1.jpg'\n",
    "    #test_img_2 = 'test_images/straight_lines2.jpg'\n",
    "    test_img_2 = 'test_images/test1.jpg'\n",
    "    img1 = plt.imread(test_img_1)\n",
    "    img2 = plt.imread(test_img_2)\n",
    "    test_img_1_undistorted = undistort(img1, mtx, dist)\n",
    "    test_img_2_undistorted = undistort(img2, mtx, dist)\n",
    "\n",
    "\n",
    "    f, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2, 2, figsize=(18, 7))\n",
    "    f.tight_layout()\n",
    "    ax1.imshow(img1)\n",
    "    ax1.set_title('Original Image 1', fontsize=10)\n",
    "    ax2.imshow(img2)\n",
    "    ax2.set_title('Original Image 2', fontsize=10)\n",
    "    ax3.imshow(test_img_1_undistorted)\n",
    "    ax3.set_title('Undistorted Image 1', fontsize=10)\n",
    "    ax4.imshow(test_img_2_undistorted)\n",
    "    ax4.set_title('Undistorted Image 2', fontsize=10)\n",
    "    plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.05)\n",
    "\n",
    "    plt.imsave('output_images/straight_lines1_undist.jpg',test_img_1_undistorted )\n",
    "    plt.imsave('output_images/straight_lines2_undist.jpg',test_img_2_undistorted )\n",
    "    \n",
    "    # Save the full figure...\n",
    "    f.savefig('output_images/straight_lines_undist.jpg')\n",
    "\n",
    "#writeup_plot_undistort_straight_lines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing perspective transform with color images of roads\n",
    "please uncomment the last line to visualize the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeup_plot_bird_eye_view_straight_lines():\n",
    "    '''\n",
    "    method: generates one image for the project writeup showing the \n",
    "    undistort and warp result applied to the two test images with straight lines\n",
    "    the image show the trapezoid shape I chose as reference.\n",
    "    '''      \n",
    "    #open first image\n",
    "    filename1 = 'output_images/straight_lines1_undist.jpg'\n",
    "    image1 = plt.imread(filename1)\n",
    "    src = getWarpSrcPts(image1) \n",
    "    dst = getWarpDstPts(image1) \n",
    "\n",
    "    warped1,M,invM = warper(image1, src, dst)\n",
    "    plt.imsave('output_images/straight_lines1_warped.jpg',warped1)\n",
    "    #left reference line warped image\n",
    "    cv2.line(warped1, (dst[2][0], dst[2][1]), (dst[3][0], dst[3][1]), [0,255,0], 4)\n",
    "    cv2.circle(warped1, (dst[2][0], dst[2][1]), 2, [0,255,0], 12)\n",
    "    cv2.circle(warped1, (dst[3][0], dst[3][1]), 2, [0,255,0], 12)\n",
    "\n",
    "    #right reference line warped image\n",
    "    cv2.line(warped1, (dst[0][0], dst[0][1]), (dst[1][0], dst[1][1]), [255,0,0], 4)\n",
    "    cv2.circle(warped1, (dst[0][0], dst[0][1]), 2, [255,0,0], 12)\n",
    "    cv2.circle(warped1, (dst[1][0], dst[1][1]), 2, [255,0,0], 12)\n",
    "\n",
    "    #left reference line original image\n",
    "    cv2.line(image1, (src[2][0], src[2][1]), (src[3][0], src[3][1]), [0,255,0], 4) \n",
    "    cv2.line(image1, (src[2][0], src[2][1]), (src[1][0], src[1][1]), [0,0,255], 4)\n",
    "    cv2.line(image1, (src[3][0], src[3][1]), (src[0][0], src[0][1]), [0,0,255], 4) \n",
    "    cv2.circle(image1, (src[2][0], src[2][1]), 2, [0,255,0], 12)\n",
    "    cv2.circle(image1, (src[3][0], src[3][1]), 2, [0,255,0], 12)\n",
    "\n",
    "    #right reference line original image\n",
    "    cv2.line(image1, (src[0][0], src[0][1]), (src[1][0], src[1][1]), [255,0,0], 4)\n",
    "    cv2.circle(image1, (src[0][0], src[0][1]), 2, [255,0,0], 12)\n",
    "    cv2.circle(image1, (src[1][0], src[1][1]), 2, [255,0,0], 12)\n",
    "\n",
    "    filename2 = 'output_images/straight_lines2_undist.jpg'\n",
    "    image2 = plt.imread(filename2)\n",
    "    src = getWarpSrcPts(image2) \n",
    "    dst = getWarpDstPts(image2) \n",
    "\n",
    "    warped2,M,invM = warper(image2, src, dst)\n",
    "\n",
    "    plt.imsave('output_images/straight_lines2_warped.jpg',warped2)\n",
    "    #left reference line warped image\n",
    "    cv2.line(warped2, (dst[2][0], dst[2][1]), (dst[3][0], dst[3][1]), [0,255,0], 4)\n",
    "    cv2.circle(warped2, (dst[2][0], dst[2][1]), 2, [0,255,0], 12)\n",
    "    cv2.circle(warped2, (dst[3][0], dst[3][1]), 2, [0,255,0], 12)\n",
    "\n",
    "    #right reference line warped image\n",
    "    cv2.line(warped2, (dst[0][0], dst[0][1]), (dst[1][0], dst[1][1]), [255,0,0], 4)\n",
    "    cv2.circle(warped2, (dst[0][0], dst[0][1]), 2, [255,0,0], 12)\n",
    "    cv2.circle(warped2, (dst[1][0], dst[1][1]), 2, [255,0,0], 12)\n",
    "\n",
    "    #left reference line original image\n",
    "    cv2.line(image2, (src[2][0], src[2][1]), (src[3][0], src[3][1]), [0,255,0], 4) \n",
    "    cv2.line(image2, (src[2][0], src[2][1]), (src[1][0], src[1][1]), [0,0,255], 4)\n",
    "    cv2.line(image2, (src[3][0], src[3][1]), (src[0][0], src[0][1]), [0,0,255], 4) \n",
    "    cv2.circle(image2, (src[2][0], src[2][1]), 2, [0,255,0], 12)\n",
    "    cv2.circle(image2, (src[3][0], src[3][1]), 2, [0,255,0], 12)\n",
    "\n",
    "    #right reference line original image\n",
    "    cv2.line(image2, (src[0][0], src[0][1]), (src[1][0], src[1][1]), [255,0,0], 4)\n",
    "    cv2.circle(image2, (src[0][0], src[0][1]), 2, [255,0,0], 12)\n",
    "    cv2.circle(image2, (src[1][0], src[1][1]), 2, [255,0,0], 12)\n",
    "\n",
    "\n",
    "    f, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2, 2, figsize=(18, 7))\n",
    "    f.tight_layout()\n",
    "    ax1.imshow(image1)\n",
    "    ax1.set_title('Original image 1 with reference lines and points', fontsize=10)\n",
    "    ax2.imshow(image2)\n",
    "    ax2.set_title('Original image 2 with reference lines and points', fontsize=10)\n",
    "    ax3.imshow(warped1)\n",
    "    ax3.set_title('Img 1 Undistorted and warped. \\nThe lines should be parallel in the warped space.', fontsize=10)\n",
    "\n",
    "    ax4.imshow(warped2)\n",
    "    ax4.set_title('Img2 Undistorted and warped. \\nThe lines should be parallel in the warped space.', fontsize=10)\n",
    "    plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.05,wspace = None,hspace = 0.3)\n",
    "\n",
    "\n",
    "    # Save the full figure...\n",
    "    f.savefig('output_images/warped_straight_lines.jpg')\n",
    "\n",
    "\n",
    "#writeup_plot_bird_eye_view_straight_lines()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test unwarp paramenters for challenge video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeup_plot_bird_eye_view_straight_lines_challenge():\n",
    "    '''\n",
    "    method: generates one image for the project writeup showing the \n",
    "    undistort and warp result applied to one frame from the challenge video\n",
    "    in fact it seems that the reference points I chose for the project video\n",
    "    do not work well for the challenge video\n",
    "    '''      \n",
    "    \n",
    "    filename = 'test_images/challenge_video/test373.jpg'\n",
    "\n",
    "    img = plt.imread(filename)\n",
    "\n",
    "    image1 = undistort(img, mtx, dist)\n",
    "\n",
    "    src = getWarpSrcPtsChallenge(image1)\n",
    "    #src = getWarpSrcPts(image1) \n",
    "    dst = getWarpDstPts(image1) \n",
    "\n",
    "    warped1,M,invM = warper(image1, src, dst)\n",
    "    #plt.imsave('output_images/straight_lines1_warped.jpg',warped1)\n",
    "    #left reference line warped image\n",
    "    cv2.line(warped1, (dst[2][0], dst[2][1]), (dst[3][0], dst[3][1]), [0,255,0], 4)\n",
    "    cv2.circle(warped1, (dst[2][0], dst[2][1]), 2, [0,255,0], 12)\n",
    "    cv2.circle(warped1, (dst[3][0], dst[3][1]), 2, [0,255,0], 12)\n",
    "\n",
    "    #right reference line warped image\n",
    "    cv2.line(warped1, (dst[0][0], dst[0][1]), (dst[1][0], dst[1][1]), [255,0,0], 4)\n",
    "    cv2.circle(warped1, (dst[0][0], dst[0][1]), 2, [255,0,0], 12)\n",
    "    cv2.circle(warped1, (dst[1][0], dst[1][1]), 2, [255,0,0], 12)\n",
    "\n",
    "    #left reference line original image\n",
    "    cv2.line(image1, (src[2][0], src[2][1]), (src[3][0], src[3][1]), [0,255,0], 4) \n",
    "    cv2.line(image1, (src[2][0], src[2][1]), (src[1][0], src[1][1]), [0,0,255], 4)\n",
    "    cv2.line(image1, (src[3][0], src[3][1]), (src[0][0], src[0][1]), [0,0,255], 4) \n",
    "    cv2.circle(image1, (src[2][0], src[2][1]), 2, [0,255,0], 12)\n",
    "    cv2.circle(image1, (src[3][0], src[3][1]), 2, [0,255,0], 12)\n",
    "\n",
    "    #right reference line original image\n",
    "    cv2.line(image1, (src[0][0], src[0][1]), (src[1][0], src[1][1]), [255,0,0], 4)\n",
    "    cv2.circle(image1, (src[0][0], src[0][1]), 2, [255,0,0], 12)\n",
    "    cv2.circle(image1, (src[1][0], src[1][1]), 2, [255,0,0], 12)\n",
    "\n",
    "   \n",
    "    f, (ax1,ax2) = plt.subplots(1, 2, figsize=(18, 7))\n",
    "    f.tight_layout()\n",
    "    ax1.imshow(image1)\n",
    "    ax1.set_title('Original image 1 with reference lines and points', fontsize=10)\n",
    "\n",
    "    \n",
    "    ax2.imshow(warped1)\n",
    "    ax2.set_title('Undistorted and warped image. \\nThe lines should be parallel in the warped space.', fontsize=10)\n",
    "    plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.05,wspace = None,hspace = 0.3)\n",
    "\n",
    "\n",
    "    # Save the full figure...\n",
    "    #f.savefig('output_images/warped_straight_lines_challenge.jpg')\n",
    "\n",
    "\n",
    "#writeup_plot_bird_eye_view_straight_lines_challenge()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing: apply color, gradient, undistort and warp to other test images (curved lines)\n",
    "please uncomment the last line to visualize the images (the amount of output images depends on the folder you choose for testing. If no folder is given as input (folder_to_test = '') then the default folder is \"test_images\" (where I have added some additional images with respect to the original 6). Other possible folders are \"project_video\",\"straight_lines\" and \"challenge_video\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeup_test_colors_and_gradiends(folder=''):\n",
    "    '''\n",
    "    method: generates several images. This method is used for testing and for the \n",
    "    project writeup showing the output of all single color filters and gradients\n",
    "    but also the combined figure (logical OR of single channels) that will be used\n",
    "    later in the final pipeline\n",
    "    '''    \n",
    "    \n",
    "    root_path = 'test_images/'\n",
    "    path = root_path + folder\n",
    "\n",
    "    full_path = path + '52.jpg'\n",
    "\n",
    "\n",
    "    images = glob.glob(full_path)    \n",
    "\n",
    "    for im in images:\n",
    "        imNum = im.split(path)[1].split('.jpg')[0]\n",
    "        image = mpimg.imread(im)\n",
    "\n",
    "        warped,M,invM,src,dst = undistort_and_warp(image,challenge=True)\n",
    "        binary_or_img,bin_sobelx,bin_s_channel,bin_r_channel,bin_sobel_mag = colors_and_gradients(warped)\n",
    "\n",
    "\n",
    "        #convert the image to three channel for vis purposes\n",
    "        binary_or_img_color = cv2.cvtColor(binary_or_img, cv2.COLOR_GRAY2RGB) *255\n",
    "        #left reference line warped image\n",
    "        cv2.line(binary_or_img_color, (dst[2][0], dst[2][1]), (dst[3][0], dst[3][1]), [0,255,0], 4)\n",
    "        #right reference line warped image\n",
    "        cv2.line(binary_or_img_color, (dst[0][0], dst[0][1]), (dst[1][0], dst[1][1]), [255,0,0], 4)    \n",
    "\n",
    "        f, ((ax1,ax2,ax3),(ax4,ax5,ax6)) = plt.subplots(2, 3, figsize=(18, 7))\n",
    "        f.tight_layout()\n",
    "        ax1.imshow(warped)\n",
    "        ax1.set_title('Warped test image {}'.format(imNum), fontsize=10)\n",
    "        ax2.imshow(cv2.cvtColor(bin_s_channel, cv2.COLOR_GRAY2RGB) *255)\n",
    "        ax2.set_title('Binary saturation channel', fontsize=10)\n",
    "        ax3.imshow(cv2.cvtColor(bin_r_channel, cv2.COLOR_GRAY2RGB) *255)\n",
    "        ax3.set_title('Binary red channel', fontsize=10)\n",
    "        ax4.imshow(cv2.cvtColor(bin_sobelx, cv2.COLOR_GRAY2RGB) *255)\n",
    "        ax4.set_title('Binary sobelx channel', fontsize=10)\n",
    "        ax5.imshow(cv2.cvtColor(bin_sobel_mag, cv2.COLOR_GRAY2RGB) *255)\n",
    "        ax5.set_title('Binary sobel magnitude (x and y) channel', fontsize=10)\n",
    "        ax6.imshow(binary_or_img_color)\n",
    "        ax6.set_title('Binary OR of other channels \\n(red channel might be filtered out)', fontsize=10)    \n",
    "        plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.05,wspace = None,hspace = 0.3)\n",
    "        \n",
    "        #f.savefig('output_images/colors_gradients_examples{}.jpg'.format(imNum))\n",
    "\n",
    "folder_to_test = ''\n",
    "#folder_to_test  = 'project_video/test'\n",
    "#folder_to_test ='straight_lines/test'\n",
    "folder_to_test ='challenge_video/test'\n",
    "#writeup_test_colors_and_gradiends(folder_to_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding lane pixels with sliding windows and polynomials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_poly_lines(img_shape, left_fit,right_fit):\n",
    "    '''\n",
    "    method: build two vertical lines using the the y axis of as input coordinates \n",
    "    and the two polynomial coefficients. The coefficients must be three per line\n",
    "    (the method expects 2 deg polynomial coeffs). An exception is catched if the \n",
    "    polynomial coeffs are none or incorrect\n",
    "    input: \n",
    "        img input image \n",
    "        left_fit = polynomial coefficients for the left line\n",
    "        right_fit = polynomial coefficients for the right line\n",
    "    output:\n",
    "        left_fitx = x coordinates of the left line\n",
    "        right_fitx = x coordinates of the right line\n",
    "        ploty y coordinates (the same for both lines)\n",
    "    '''   \n",
    "    \n",
    "    # Generate y values for line evaluation\n",
    "    ploty = np.linspace(0, img_shape[0]-1, img_shape[0])\n",
    "    \n",
    "    #evaluate the lines using the coeffs\n",
    "    try:           \n",
    "        right_fitx = ploty**2*right_fit[0] + ploty*right_fit[1] + right_fit[2]\n",
    "        left_fitx = ploty**2*left_fit[0] + ploty*left_fit[1] + left_fit[2]\n",
    "    except TypeError:\n",
    "        # Avoids an error if `left` and `right_fit` are still none or incorrect\n",
    "        print('The function failed to fit a line!')\n",
    "        left_fitx = 1*ploty**2 + 1*ploty\n",
    "        right_fitx = 1*ploty**2 + 1*ploty\n",
    "\n",
    "    return left_fitx, right_fitx, ploty\n",
    "\n",
    "def fit_poly(leftx, lefty, rightx, righty,xm_per_pix,ym_per_pix):\n",
    "    '''\n",
    "    method: generates the second degree polynomial \n",
    "    coefficients for two set of points\n",
    "    input: \n",
    "        leftx = x coord of the first line\n",
    "        lefty = y coord of the first line\n",
    "        rightx = x coord of the second line\n",
    "        righty = y coord of the second line\n",
    "        xm_per_pix = pix to meters conversion  \n",
    "        ym_per_pix = pix to meters conversion \n",
    "    output:\n",
    "        left_fit = 2 degree poly coefficients for first line\n",
    "        right_fit = 2 degree poly coefficients for second line \n",
    "        left_fit_meters = 2 degree poly coefficients for first line converted\n",
    "        right_fit_meters = = 2 degree poly coefficients for second line converted\n",
    "        return -1,-1,-1,-1 if any of the input vectors is empty \n",
    "    ''' \n",
    "    #build result dictionary\n",
    "    result = {'left_fit':-1,'right_fit':-1,'left_fit_meters':-1,'right_fit_meters':-1}\n",
    "\n",
    "    if (lefty.size>0 and leftx.size>0 and righty.size>0 and rightx.size>0):\n",
    "        left_fit = np.polyfit(lefty, leftx, 2)\n",
    "        right_fit = np.polyfit(righty, rightx, 2)\n",
    "\n",
    "        left_fit_meters = np.polyfit(lefty*ym_per_pix, leftx*xm_per_pix, 2)\n",
    "        right_fit_meters = np.polyfit(righty*ym_per_pix, rightx*xm_per_pix, 2)\n",
    "\n",
    "        result['left_fit'] = left_fit\n",
    "        result['right_fit'] = right_fit\n",
    "        result['left_fit_meters'] = left_fit_meters\n",
    "        result['right_fit_meters'] = right_fit_meters\n",
    "         \n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def lane_search_with_windows(binary_image, xm_per_pix, ym_per_pix, bottom_half = False):\n",
    "    '''\n",
    "    method: takes an imput image, searches for road lanes (left and righ lane)\n",
    "    using a computational intensive sliding window method. \n",
    "    If the flag bottom_half is true, the image searches for lines in the bottom\n",
    "    half of the image only\n",
    "    input: \n",
    "        binary_image input image should be a binary image\n",
    "        xm_per_pix = pix to meters conversion  \n",
    "        ym_per_pix = pix to meters conversion        \n",
    "        bottom_half flag, Flase by default\n",
    "    output:\n",
    "        out_img input image with marked lines and windows\n",
    "        left_fitx x coordinats of the left line\n",
    "        right_fitx x coordinats of the right line\n",
    "        ploty y coordinats of the both lines line\n",
    "        left_fit 2deg polynomial coeffs of the left line\n",
    "        right_fit 2deg polynomial coeffs of the right line\n",
    "        left_fit_meters = poly coeffs converted in meters for curvature measure\n",
    "        right_fit_meters = poly coeffs converted in meters for curvature measure\n",
    "        return -1,-1,-1,-1,-1,-1,-1,-1 in case of invalid results\n",
    "    '''  \n",
    "    result = {'out_img':-1,\n",
    "             'left_fitx':-1,\n",
    "             'right_fitx':-1,\n",
    "             'ploty':-1,\n",
    "             'left_fit':-1,\n",
    "             'right_fit':-1,\n",
    "             'left_fit_meters':-1,\n",
    "             'right_fit_meters':-1,}\n",
    "    \n",
    "    # Find our lane pixels first (find both lanes using the sliding windows method)\n",
    "    # the result is the coordinates of both lanes\n",
    "    # the method returns also the output image (a binary image) that \n",
    "    # shows  the two lanes and the windows\n",
    "    leftx, lefty, rightx, righty, out_img = lane_search_vertical_windows(binary_image,bottom_half)\n",
    "\n",
    "    #Fit a second order polynomial to each using `np.polyfit` \n",
    "    fitted_poly = fit_poly(leftx, lefty, rightx, righty,xm_per_pix,ym_per_pix)\n",
    "    \n",
    "    left_fit = fitted_poly['left_fit']\n",
    "    right_fit = fitted_poly['right_fit']\n",
    "    left_fit_meters = fitted_poly['left_fit_meters']\n",
    "    right_fit_meters = fitted_poly['right_fit_meters']\n",
    "\n",
    "    #check if the poly coeffs are valid \n",
    "    if (type(left_fit) != int and left_fit.all() != -1):\n",
    "        #consider full image or only bottom half of it\n",
    "        if (bottom_half==False):\n",
    "            imsize = (binary_image.shape[0]-1, binary_image.shape[0])\n",
    "        else:\n",
    "            imsize = (binary_image.shape[0]-1, binary_image.shape[0]//2)\n",
    "\n",
    "        # Generate x and y values for plotting\n",
    "        left_fitx, right_fitx, ploty =  build_poly_lines(binary_image.shape, left_fit,right_fit)\n",
    "\n",
    "        ## Visualization ##\n",
    "        # Colors in the left and right lane regions\n",
    "        out_img[lefty, leftx] = [255, 0, 0]\n",
    "        out_img[righty, rightx] = [0, 0, 255]\n",
    "\n",
    "        result['out_img'] = out_img\n",
    "        result['left_fitx'] = left_fitx\n",
    "        result['right_fitx'] = right_fitx\n",
    "        result['ploty'] = ploty\n",
    "        result['left_fit'] = left_fit\n",
    "        result['right_fit'] = right_fit\n",
    "        result['left_fit_meters'] = left_fit_meters\n",
    "        result['right_fit_meters'] =  right_fit_meters     \n",
    "        \n",
    "    return result\n",
    "        \n",
    "    \n",
    "def lane_search_around_poly(binary_image,left_poly_fit,right_poly_fit,xm_per_pix, ym_per_pix):\n",
    "    '''\n",
    "    method: takes an imput image, searches for road lanes (left and righ lane)\n",
    "    around two given polynomials. The polynomials are expressed as poly coefficients\n",
    "    input: \n",
    "        binary_image = input image in binary format\n",
    "        left_poly_fit = coeffs of the \"left\" polynomial\n",
    "        right_poly_fit = coeffs of the \"right\" polynomial\n",
    "    output:\n",
    "        image_with_lanes = output image\n",
    "        left_fitx = x coords of the left lane line\n",
    "        right_fitx = x coords of the right lane line\n",
    "        ploty = y coords of both lane lines\n",
    "        left_fit = poly coeffs of the detected left lane line\n",
    "        right_fit = poly coeffs of the detected right lane line\n",
    "        left_fit_meters = poly coeffs converted in meters for curvature measure\n",
    "        right_fit_meters = poly coeffs converted in meters for curvature measure\n",
    "        return -1,-1,-1,-1,-1,-1,-1,-1 in case of invalid results\n",
    "    '''\n",
    "    result = {'image_with_lanes':-1,\n",
    "             'left_fitx':-1,\n",
    "             'right_fitx':-1,\n",
    "             'ploty':-1,\n",
    "             'left_fit':-1,\n",
    "             'right_fit':-1,\n",
    "             'left_fit_meters':-1,\n",
    "             'right_fit_meters':-1,}   \n",
    "    \n",
    "    # HYPERPARAMETERS\n",
    "    margin = 30\n",
    "\n",
    "    # Grab activated pixels from the binary image given as input\n",
    "    nonzero = binary_image.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    \n",
    "    #search for the indices of activated pixels around the two polys\n",
    "    left_lane_inds = ((nonzerox <= ((nonzeroy**2)*left_poly_fit[0]+nonzeroy*left_poly_fit[1]+left_poly_fit[2])+margin)&\n",
    "                      (nonzerox > ((nonzeroy**2)*left_poly_fit[0]+nonzeroy*left_poly_fit[1]+left_poly_fit[2])-margin))\n",
    "    \n",
    "    right_lane_inds = ((nonzerox <= ((nonzeroy**2)*right_poly_fit[0]+nonzeroy*right_poly_fit[1]+right_poly_fit[2])+margin)&\n",
    "                      (nonzerox > ((nonzeroy**2)*right_poly_fit[0]+nonzeroy*right_poly_fit[1]+right_poly_fit[2])-margin))\n",
    "    \n",
    "    # extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds] \n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds]\n",
    "\n",
    "\n",
    "    # Fit new polynomials\n",
    "    fitted_poly = fit_poly(leftx, lefty, rightx, righty,xm_per_pix,ym_per_pix)\n",
    "    \n",
    "    left_fit = fitted_poly['left_fit']\n",
    "    right_fit = fitted_poly['right_fit']\n",
    "    left_fit_meters = fitted_poly['left_fit_meters']\n",
    "    right_fit_meters = fitted_poly['right_fit_meters']\n",
    "    \n",
    "    #check if the poly coeffs are valid \n",
    "    if (type(left_fit) != int and left_fit.all() != -1):\n",
    "        #build new lane lines\n",
    "        left_fitx, right_fitx, ploty = build_poly_lines(binary_image.shape, left_fit,right_fit)\n",
    "\n",
    "        ## Visualization ##\n",
    "        # Create an image to draw on and an image to show the selection window\n",
    "        out_img = np.dstack((binary_image, binary_image, binary_image))*255\n",
    "        window_img = np.zeros_like(out_img)\n",
    "\n",
    "        # Color in left and right line pixels\n",
    "        out_img[nonzeroy[left_lane_inds], nonzerox[left_lane_inds]] = [255, 0, 0]\n",
    "        out_img[nonzeroy[right_lane_inds], nonzerox[right_lane_inds]] = [0, 0, 255]\n",
    "\n",
    "        # Generate a polygon to illustrate the search window area\n",
    "        # And recast the x and y points into usable format for cv2.fillPoly()\n",
    "        left_line_window1 = np.array([np.transpose(np.vstack([left_fitx-margin, ploty]))])\n",
    "        left_line_window2 = np.array([np.flipud(np.transpose(np.vstack([left_fitx+margin, \n",
    "                                  ploty])))])\n",
    "        left_line_pts = np.hstack((left_line_window1, left_line_window2))\n",
    "\n",
    "        right_line_window1 = np.array([np.transpose(np.vstack([right_fitx-margin, ploty]))])\n",
    "        right_line_window2 = np.array([np.flipud(np.transpose(np.vstack([right_fitx+margin, \n",
    "                                  ploty])))])\n",
    "        right_line_pts = np.hstack((right_line_window1, right_line_window2))\n",
    "\n",
    "        # Draw the lane onto the warped blank image\n",
    "        cv2.fillPoly(window_img, np.int_([left_line_pts]), (0,255, 0))\n",
    "        cv2.fillPoly(window_img, np.int_([right_line_pts]), (0,255, 0))\n",
    "        image_with_lanes = cv2.addWeighted(out_img, 1, window_img, 0.3, 0)\n",
    "        \n",
    "        result['image_with_lanes'] = out_img\n",
    "        result['left_fitx'] = left_fitx\n",
    "        result['right_fitx'] = right_fitx\n",
    "        result['ploty'] = ploty\n",
    "        result['left_fit'] = left_fit\n",
    "        result['right_fit'] = right_fit\n",
    "        result['left_fit_meters'] = left_fit_meters\n",
    "        result['right_fit_meters'] =  right_fit_meters \n",
    "        \n",
    "    return result\n",
    "\n",
    "\n",
    "def measure_curvature_in_meters(ym_per_pix,xm_per_pix,ploty,left_poly_fit,right_poly_fit):\n",
    "    '''\n",
    "    method: Calculates the curvature of polynomial functions in meters.\n",
    "    input: \n",
    "        ym_per_pix = conversion rate for y axis\n",
    "        xm_per_pix = conversion rate for x axis\n",
    "        ploty = y coords of the lane lines\n",
    "        left_poly_fit = poly coeffs left lane line\n",
    "        right_poly_fit = poly coeffs right lane line\n",
    "    output:\n",
    "        left_curverad = radius of left curve in meters\n",
    "        right_curverad = radius of right curve in meters\n",
    "    '''   \n",
    "    # We'll choose the maximum y-value, corresponding to the bottom of the image\n",
    "    y_eval = np.max(ploty)\n",
    "    \n",
    "    #calculation of R_curve (radius of curvature) for each line\n",
    "    left_curverad = ((1 + (2*left_poly_fit[0]*y_eval*ym_per_pix + left_poly_fit[1])**2)**1.5) / np.absolute(2*left_poly_fit[0])\n",
    "    right_curverad = ((1 + (2*right_poly_fit[0]*y_eval*ym_per_pix + right_poly_fit[1])**2)**1.5) / np.absolute(2*right_poly_fit[0])\n",
    "    \n",
    "    return left_curverad, right_curverad\n",
    "\n",
    "def lane_search_vertical_windows(binary_image,bottom_half=False):\n",
    "    '''\n",
    "    method: apply the vertical sliding windows process to find \n",
    "    lane lines within the given input image (that should be a binary \n",
    "    image). If the flag is true, the vertical windows stop at half of \n",
    "    the image.\n",
    "    input: \n",
    "        binary_image = input image\n",
    "        bottom_half = flag, if true the algo search only within the bottom\n",
    "                    half of the image (which is the part of the image closest \n",
    "                    to the camera)\n",
    "    output:\n",
    "        leftx = x coords of the left lane line\n",
    "        lefty = y coords of the left lane line\n",
    "        rightx = x coords of the right lane line\n",
    "        righty = y coords of the right lane line\n",
    "        out_img = output image with lanes marked\n",
    "    '''\n",
    "    \n",
    "    # Take a histogram of the bottom half of the image\n",
    "    histogram = np.sum(binary_image[binary_image.shape[0]//2:,:], axis=0)\n",
    "    \n",
    "    # Create an output image to draw on and visualize the result\n",
    "    out_img = np.dstack((binary_image, binary_image, binary_image))\n",
    "    \n",
    "    # Find the peak of the left and right halves of the histogram\n",
    "    # These will be the starting point for the left and right lines\n",
    "    midpoint = np.int(histogram.shape[0]//2)\n",
    "    leftx_base = np.argmax(histogram[:midpoint])\n",
    "    rightx_base = np.argmax(histogram[midpoint:]) + midpoint\n",
    "\n",
    "    \n",
    "    # HYPERPARAMETERS\n",
    "    # Choose the number of sliding windows\n",
    "    nwindows = 20\n",
    "    # Set the width of the windows +/- margin\n",
    "    margin = 30\n",
    "    # Set minimum number of pixels found to recenter window\n",
    "    minpix = 30\n",
    "\n",
    "    # Set height of windows - based on nwindows above and image shape\n",
    "    # if the flag is true, the process should stop at half of the image\n",
    "    # (with respect to the y axis)\n",
    "    if (bottom_half==False):\n",
    "        window_height = np.int(binary_image.shape[0]//nwindows)\n",
    "    else:\n",
    "        window_height = np.int((binary_image.shape[0]//2)//nwindows)\n",
    "    \n",
    "    # Identify the x and y positions of all nonzero pixels in the image\n",
    "    nonzero = binary_image.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    \n",
    "    # Current positions to be updated later for each window in nwindows\n",
    "    leftx_current = leftx_base\n",
    "    rightx_current = rightx_base\n",
    "\n",
    "    # Create empty lists to receive left and right lane pixel indices\n",
    "    left_lane_inds = []\n",
    "    right_lane_inds = []\n",
    "\n",
    "    # Step through the windows one by one\n",
    "    for window in range(nwindows):\n",
    "        # Identify window boundaries in x and y (and right and left)\n",
    "        win_y_low = binary_image.shape[0] - (window+1)*window_height\n",
    "        win_y_high = binary_image.shape[0] - window*window_height\n",
    "        \n",
    "        #Find the four below boundaries of the window\n",
    "        win_xleft_low = leftx_current - margin  \n",
    "        win_xleft_high = leftx_current + margin  \n",
    "        \n",
    "        win_xright_low = rightx_current - margin\n",
    "        win_xright_high = rightx_current + margin\n",
    "        \n",
    "        # Draw the windows on the visualization image\n",
    "        cv2.rectangle(out_img,(win_xleft_low,win_y_low),\n",
    "        (win_xleft_high,win_y_high),(0,255,0), 2) \n",
    "        cv2.rectangle(out_img,(win_xright_low,win_y_low),\n",
    "        (win_xright_high,win_y_high),(0,255,0), 2) \n",
    "        \n",
    "        # Identify the nonzero pixels in x and y within the window\n",
    "        good_left_inds = ((nonzerox >= win_xleft_low) & (nonzerox <win_xleft_high) &\n",
    "        (nonzeroy>=win_y_low) & (nonzeroy <win_y_high)).nonzero()[0]\n",
    "        good_right_inds = ((nonzerox >=win_xright_low)&(nonzerox <win_xright_high)&\n",
    "                           (nonzeroy>=win_y_low) & (nonzeroy <win_y_high)).nonzero()[0]\n",
    "        \n",
    "        # Append these indices to the lists\n",
    "        left_lane_inds.append(good_left_inds)\n",
    "        right_lane_inds.append(good_right_inds)\n",
    "        \n",
    "        #If you found > minpix pixels, recenter next window\n",
    "        #(`right` or `leftx_current`) on their mean position\n",
    "        if len(good_left_inds) > minpix:\n",
    "            leftx_current = np.int(np.mean(nonzerox[good_left_inds]))\n",
    "        if len(good_right_inds) > minpix:        \n",
    "            rightx_current = np.int(np.mean(nonzerox[good_right_inds]))\n",
    "\n",
    "    # Concatenate the arrays of indices (previously was a list of lists of pixels)\n",
    "    try:\n",
    "        left_lane_inds = np.concatenate(left_lane_inds)\n",
    "        right_lane_inds = np.concatenate(right_lane_inds)\n",
    "    except ValueError:\n",
    "        # Avoids an error if the above is not implemented fully\n",
    "        pass\n",
    "\n",
    "    # Extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds] \n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds]\n",
    "\n",
    "    return leftx, lefty, rightx, righty, out_img\n",
    "\n",
    "def car_lanes_offset(left,right,imwidth):\n",
    "    '''\n",
    "    method: calculate the car offset with respect to the lanes.\n",
    "    Positive offsets means that the car is on the left of the \n",
    "    middle lane\n",
    "    input: \n",
    "        left = x reference coordinate from the left lane\n",
    "        right = x reference coordinate from the right lane\n",
    "        imwidth = width of the image\n",
    "    output:\n",
    "        offset = car offset\n",
    "        lane_width = lane width\n",
    "    '''\n",
    "    \n",
    "    #the middle point of the image is half of the image width\n",
    "    imMid = imwidth//2\n",
    "    #calculate the middle point between lanes\n",
    "    laneMid = left+(right-left)//2\n",
    "    #width of the lane\n",
    "    lane_width = right-left\n",
    "\n",
    "    #calculate offset with respect to the middle of the image (which\n",
    "    # corresponds ot the position of the camera)\n",
    "    offset = laneMid-imMid\n",
    "    return offset,lane_width\n",
    "\n",
    "def color_driving_space(grayImg,left_fitx, right_fitx, ploty):\n",
    "    '''\n",
    "    method: use cv2.fillpoly to color the driving space between two lines.\n",
    "    The driving space is the area in front of the car between the detected lines.\n",
    "    input: \n",
    "        grayImg = input image\n",
    "        left_fitx = x coords of the left lane line\n",
    "        right_fitx = x coords of the right lane line\n",
    "        ploty = y coords of both lane lines\n",
    "    output:\n",
    "        driving_space = image with colored driving space inlcuding the two lines\n",
    "    '''\n",
    "    #get height and width of the image\n",
    "    h, w = grayImg.shape[:2]\n",
    "\n",
    "    #get the \"driving space\" between the lines and the points of the lines themselves\n",
    "    pointsL = np.array([None])\n",
    "    pointsR = np.array([None])\n",
    "    pointsL = np.array([[[xi, yi]] for xi, yi in zip(left_fitx, ploty) if (0<=xi<w and 0<=yi<h)]).astype(np.int32)\n",
    "    pointsR = np.array([[[xi, yi]] for xi, yi in zip(right_fitx, ploty) if (0<=xi<w and 0<=yi<h)]).astype(np.int32)\n",
    "    pointsR = np.flipud(pointsR)\n",
    "    #driving space points\n",
    "    points = np.concatenate((pointsL, pointsR))\n",
    "\n",
    "    driving_space = grayImg.copy()\n",
    "    driving_space = cv2.cvtColor(driving_space,cv2.COLOR_GRAY2RGB)\n",
    "    #color driving space\n",
    "    cv2.fillPoly(driving_space, [points], color=[0,255,0])\n",
    "    #add left line overlay\n",
    "    cv2.polylines(driving_space, [pointsL], color=[255,0,0], isClosed = False,thickness = 20)\n",
    "    #add right line overlay\n",
    "    cv2.polylines(driving_space, [pointsR], color=[255,0,0], isClosed = False,thickness = 20)   \n",
    "    \n",
    "    return driving_space\n",
    "\n",
    "def warp_binary_img(image,challenge=False):\n",
    "    '''\n",
    "    method: apply undistort, perspective transform, \n",
    "    color filtering and gradients to the input image\n",
    "    input: \n",
    "        image = input image to process\n",
    "    output:\n",
    "        warped = undistorted and warped image\n",
    "        M = perspective transform matrix\n",
    "        invM = inverse perspective transform matrix\n",
    "        binary_or_img = binary image\n",
    "    '''    \n",
    "    #apply undistort and perspective transform\n",
    "    warped,M,invM,src,dst = undistort_and_warp(image,challenge)\n",
    "    #apply color filters and gradients\n",
    "    binary_or_img,bin_sobelx,bin_s_channel,bin_r_channel,bin_sobel_mag = colors_and_gradients(warped)\n",
    "    \n",
    "    return warped,M,invM,binary_or_img\n",
    "\n",
    "\n",
    "def add_tags(image,leftcurv,rightcurv,left,right,xm_per_pix,testing=False):\n",
    "    '''\n",
    "    method: helper method to add text to the input image.     \n",
    "    input: \n",
    "        image = input image to process\n",
    "        testing = if true, show more data than needed for final project\n",
    "        leftcurv = curvature radius of left line\n",
    "        rightcurv = curvature radius of right line\n",
    "        left = x coord of one point on the left lane line\n",
    "        right = x coord of one point on the right lane line\n",
    "    output:\n",
    "        none\n",
    "    '''    \n",
    "    car_offset,laneWidth = car_lanes_offset(left,right,image.shape[1])\n",
    "    leftTxt = \"Left curvr:{:.2f}[m]\".format(leftcurv)    \n",
    "    rigtTxt = \"Right curvr:{:.2f}[m]\".format(rightcurv)\n",
    "    avgTxt = \"Average curvr:{:.2f}[m]\".format((leftcurv+rightcurv)/2)\n",
    "    if car_offset>0:\n",
    "        offsetTxt = \"Car offset:{:.2f}[m] to the left\".format(car_offset*xm_per_pix)    \n",
    "    elif car_offset<0:\n",
    "        offsetTxt = \"Car offset:{:.2f}[m] to the right\".format(abs(car_offset*xm_per_pix))    \n",
    "    else:\n",
    "        offsetTxt = \"Car offset:{:.2f}[m]\".format(car_offset*xm_per_pix)    \n",
    "    \n",
    "    widthtTxt =\"Lane Width:{:.2f}[m]\".format(laneWidth*xm_per_pix)\n",
    "    \n",
    "    position = (400,100)\n",
    "    cv2.putText(\n",
    "         image, #numpy array on which text is written\n",
    "         avgTxt, #text\n",
    "         position, #position at which writing has to start\n",
    "         cv2.FONT_HERSHEY_SIMPLEX, #font family\n",
    "         1, #font size\n",
    "         (255, 255, 255, 255), #font color\n",
    "         3) #font stroke \n",
    "    position = (400,140)\n",
    "    cv2.putText(\n",
    "         image, #numpy array on which text is written\n",
    "         offsetTxt, #text\n",
    "         position, #position at which writing has to start\n",
    "         cv2.FONT_HERSHEY_SIMPLEX, #font family\n",
    "         1, #font size\n",
    "         (255, 255, 255, 255), #font color\n",
    "         3) #font stroke \n",
    "    if (testing):\n",
    "        position = (400,180)    \n",
    "        cv2.putText(\n",
    "             image, #numpy array on which text is written\n",
    "             leftTxt, #text\n",
    "             position, #position at which writing has to start\n",
    "             cv2.FONT_HERSHEY_SIMPLEX, #font family\n",
    "             1, #font size\n",
    "             (255, 255, 255, 255), #font color\n",
    "             3) #font stroke \n",
    "        position = (400,220)  \n",
    "        cv2.putText(\n",
    "             image, #numpy array on which text is written\n",
    "             rigtTxt, #text\n",
    "             position, #position at which writing has to start\n",
    "             cv2.FONT_HERSHEY_SIMPLEX, #font family\n",
    "             1, #font size\n",
    "             (255, 255, 255, 255), #font color\n",
    "             3) #font stroke \n",
    "        position = (400,260)\n",
    "        cv2.putText(\n",
    "             image, #numpy array on which text is written\n",
    "             widthtTxt, #text\n",
    "             position, #position at which writing has to start\n",
    "             cv2.FONT_HERSHEY_SIMPLEX, #font family\n",
    "             1, #font size\n",
    "             (255, 255, 255, 255), #font color\n",
    "             3) #font stroke \n",
    "\n",
    " \n",
    "def averaging_poly(lines):\n",
    "    '''\n",
    "    method: calculate the average of the poly coeff of the two lane lines\n",
    "    using the lines in the buffer\n",
    "    input: \n",
    "        lines (tuple of two lines)\n",
    "    output:\n",
    "        avg_left = left line as average of the poly coeffs\n",
    "        avg_right = right line as average of the poly coeffs\n",
    "    '''     \n",
    "    left_coeffs = []\n",
    "    avg_left = []\n",
    "    right_coeffs = []\n",
    "    avg_right = []\n",
    "    \n",
    "    #unpack lines and extract values\n",
    "    for l,r in lines:\n",
    "        left_coeffs.append(l.getLatestFit())\n",
    "        right_coeffs.append(r.getLatestFit())\n",
    "        \n",
    "    #calculate means\n",
    "    avg_left = np.mean(np.stack(left_coeffs), axis=0)\n",
    "    avg_right = np.mean(np.stack(right_coeffs), axis=0)\n",
    "\n",
    "    return avg_left, avg_right\n",
    "\n",
    "def averaging_curvatures(lines):\n",
    "    '''\n",
    "    method: calculate the average curvatures of each line using\n",
    "    the lines in the buffer    \n",
    "    input: \n",
    "        lines (tuple of two lines)\n",
    "    output:\n",
    "        avg_left_cr = average curvature radius of left line\n",
    "        avg_right_cr = average curvature radius of right line\n",
    "    '''     \n",
    "    left_cr = []\n",
    "    avg_left = []\n",
    "    right_cr = []\n",
    "    avg_right = []\n",
    "    \n",
    "    #unpack lines and extract values\n",
    "    for l,r in lines:\n",
    "        left_cr.append(l.getCurvature())\n",
    "        right_cr.append(r.getCurvature())\n",
    "        \n",
    "    #calculate means\n",
    "    avg_left = np.mean(np.stack(left_cr), axis=0)\n",
    "    avg_right = np.mean(np.stack(right_cr), axis=0)\n",
    "\n",
    "    return avg_left, avg_right\n",
    "\n",
    "class Line():\n",
    "    def __init__(self):\n",
    "        # was the line detected in the last iteration?\n",
    "        self.detected = False  \n",
    "        # x values of the last n fits of the line\n",
    "        self.recent_xfitted = [] \n",
    "        #average x values of the fitted line over the last n iterations\n",
    "        self.bestx = None     \n",
    "        #polynomial coefficients averaged over the last n iterations\n",
    "        self.best_fit = None  \n",
    "        #polynomial coefficients for the most recent fit\n",
    "        self.current_fit = [np.array([False])]  \n",
    "        #radius of curvature of the line in some units\n",
    "        self.radius_of_curvature = None \n",
    "        #distance in meters of vehicle center from the line\n",
    "        self.line_base_pos = None \n",
    "        #difference in fit coefficients between last and new fits\n",
    "        self.diffs = np.array([0,0,0], dtype='float') \n",
    "\n",
    "        #x values for detected line pixels\n",
    "        self.allx = None  \n",
    "        #y values for detected line pixels\n",
    "        self.ally = None\n",
    "    \n",
    "    def setDetectedFlag(self, flag):\n",
    "        self.detected = flag\n",
    "        \n",
    "    def getDetectedFlag(self):\n",
    "        return self.detected\n",
    "        \n",
    "    def setLatestFitx(self,fitx):\n",
    "        self.recent_xfitted = fitx\n",
    "        \n",
    "    def getLatestFitx(self):\n",
    "        return self.recent_xfitted \n",
    "    \n",
    "    def setLatestFit(self,fit_coeffs):\n",
    "        self.current_fit = fit_coeffs\n",
    "        \n",
    "    def getLatestFit(self):\n",
    "        return self.current_fit     \n",
    "\n",
    "    def setCurvature(self,curv):\n",
    "        self.radius_of_curvature = curv\n",
    "        \n",
    "    def getCurvature(self):\n",
    "        return self.radius_of_curvature    \n",
    "\n",
    "def getDequeData(d):\n",
    "    return [data for data in d]\n",
    "\n",
    "def get_last_lines(d):\n",
    "    return d[-1]\n",
    "\n",
    "def process_frame(image):\n",
    "    \n",
    "    '''\n",
    "    method: main method to process video frames\n",
    "    input: \n",
    "        image = current frame to process\n",
    "    output:\n",
    "        procesed_image \n",
    "    '''    \n",
    "    #use a global variable for a lane buffer\n",
    "    global LinesDeque\n",
    "\n",
    "    # Define conversions in x and y from pixels space to meters\n",
    "    ym_per_pix = 15/720 # meters per pixel in y dimension based on visual observation\n",
    "    xm_per_pix = 3.7/600 # meters per pixel in x dimension based on visual observation\n",
    "    \n",
    "    \n",
    "    #get bird eye view of the frame\n",
    "    warped,M,invM,binary_warped = warp_binary_img(image)\n",
    "\n",
    "    #variables\n",
    "    valid_l_fit_x = None\n",
    "    valid_l_cr = None\n",
    "    valid_r_fit_x = None\n",
    "    valid_r_cr = None\n",
    "\n",
    "    # if the buffer is full we can start smoothing\n",
    "    # lines based on the last detected lanes. The buffer is a ring\n",
    "    # buffer so that older fralines are always replaced by recent ones\n",
    "    if len(LinesDeque) == max_queue_size:\n",
    "\n",
    "        # get the last poly values (averaged across \n",
    "        # the lines in the buffer)\n",
    "        lines = getDequeData(LinesDeque)\n",
    "        left_avg_coeffs,right_avg_coeffs = averaging_poly(lines)\n",
    "        left_avg_cr,right_avg_cr = averaging_curvatures(lines)\n",
    "         \n",
    "        #use the poly coeffs from last line to search new lines\n",
    "        find_lanes = lane_search_around_poly(binary_warped,left_avg_coeffs,right_avg_coeffs,xm_per_pix, ym_per_pix)\n",
    "\n",
    "        result = find_lanes['image_with_lanes']\n",
    "        \n",
    "        #check if new line exists\n",
    "        if (type(result) != int and result.all() != -1):\n",
    "            left_fitx = find_lanes['left_fitx']\n",
    "            right_fitx = find_lanes['right_fitx']\n",
    "            ploty = find_lanes['ploty']\n",
    "            left_fit = find_lanes['left_fit']\n",
    "            right_fit = find_lanes['right_fit']\n",
    "            lfit_meters = find_lanes['left_fit_meters']\n",
    "            rfit_meters = find_lanes['right_fit_meters']            \n",
    "            \n",
    "            #decide if the new polynomial is close enough to the previous lines\n",
    "            coeff_left = [left_avg_coeffs,left_fit]\n",
    "            coeff_right = [right_avg_coeffs,right_fit]\n",
    "                       \n",
    "            coeff_dists_left = scipy.spatial.distance_matrix(coeff_left, coeff_left)\n",
    "            coeff_dists_right = scipy.spatial.distance_matrix(coeff_right, coeff_right)\n",
    "            #calculate difference from average lines\n",
    "            poly_difference = np.mean([coeff_dists_left[0][1],coeff_dists_right[0][1]])\n",
    "\n",
    "            #calculate offset and lanewidth\n",
    "            offset,lanewidth = car_lanes_offset(left_fitx[-1],right_fitx[-1],warped.shape[1])\n",
    "            \n",
    "        else:\n",
    "            #if current line does not exist, use last one\n",
    "            last_left,last_right = get_last_lines(LinesDeque)\n",
    "                      \n",
    "            #decide if the new polynomial is close enough to the previous lines\n",
    "            coeff_left = [left_avg_coeffs,last_left.getLatestFit()]\n",
    "            coeff_right = [right_avg_coeffs,last_right.getLatestFit()]           \n",
    "            \n",
    "            coeff_dists_left = scipy.spatial.distance_matrix(coeff_left, coeff_left)\n",
    "            coeff_dists_right = scipy.spatial.distance_matrix(coeff_right, coeff_right)\n",
    "            \n",
    "            #calculate difference from average lines\n",
    "            poly_difference = np.mean([coeff_dists_left[0][1],coeff_dists_right[0][1]])\n",
    "            \n",
    "            poly_fitx_last_left = last_left.getLatestFitx()\n",
    "            poly_fitx_last_right = last_right.getLatestFitx()\n",
    "            \n",
    "            #calculate offset and lanewidth\n",
    "            offset,lanewidth = car_lanes_offset(poly_fitx_last_left[-1],poly_fitx_last_right[-1],warped.shape[1])           \n",
    "            \n",
    "            #get values form last line\n",
    "            left_fitx =  poly_fitx_last_left\n",
    "            right_fitx = poly_fitx_last_right\n",
    "            left_fit = last_left.getLatestFit()\n",
    "            right_fit = last_right.getLatestFitx()\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        #conversions\n",
    "        lanewidth = lanewidth*xm_per_pix\n",
    "        offset = offset*xm_per_pix\n",
    "        \n",
    "        #all conditions must be satisfied to accept the line\n",
    "        lanewidth_condition = lanewidth >= 3.3 and lanewidth < 4.2\n",
    "        offset_condition = abs(offset) < 0.45\n",
    "        poly_condition = poly_difference < 30\n",
    "\n",
    "        if  lanewidth_condition and offset_condition and poly_condition:\n",
    "            #good enough\n",
    "            valid_l_fit_x = left_fitx\n",
    "            valid_r_fit_x = right_fitx\n",
    "            valid_left_fit = left_fit\n",
    "            valid_right_fit = right_fit\n",
    "            left=Line()\n",
    "            right=Line()\n",
    "            left.setDetectedFlag(False)\n",
    "            right.setDetectedFlag(False)\n",
    "            \n",
    "        else:\n",
    "            #else empty the buffer, search line using sliding windows and append it\n",
    "            LinesDeque.clear()\n",
    "            #try detecting the lane on the bottom of the image\n",
    "            find_lanes = lane_search_with_windows(binary_warped,xm_per_pix,ym_per_pix,bottom_half=False)\n",
    "\n",
    "            result = find_lanes['out_img']\n",
    "            valid_l_fit_x = find_lanes['left_fitx']\n",
    "            valid_r_fit_x = find_lanes['right_fitx']\n",
    "            ploty = find_lanes['ploty']\n",
    "            valid_left_fit = find_lanes['left_fit']\n",
    "            valid_right_fit = find_lanes['right_fit']\n",
    "            lfit_meters = find_lanes['left_fit_meters']\n",
    "            rfit_meters = find_lanes['right_fit_meters']\n",
    "            \n",
    "            left=Line()\n",
    "            right=Line()\n",
    "            left.setDetectedFlag(True)\n",
    "            right.setDetectedFlag(True)\n",
    "          \n",
    "    else:\n",
    "        #fill the buffer with lines detected with the sliding windows method\n",
    "        find_lanes = lane_search_with_windows(binary_warped,xm_per_pix,ym_per_pix,bottom_half=False)\n",
    "\n",
    "        result = find_lanes['out_img']\n",
    "        valid_l_fit_x = find_lanes['left_fitx']\n",
    "        valid_r_fit_x = find_lanes['right_fitx']\n",
    "        ploty = find_lanes['ploty']\n",
    "        valid_left_fit = find_lanes['left_fit']\n",
    "        valid_right_fit = find_lanes['right_fit']\n",
    "        lfit_meters = find_lanes['left_fit_meters']\n",
    "        rfit_meters = find_lanes['right_fit_meters']\n",
    "        \n",
    "        left=Line()\n",
    "        right=Line()\n",
    "        left.setDetectedFlag(True)\n",
    "        right.setDetectedFlag(True)\n",
    "    \n",
    "    #measure curvature radius\n",
    "    valid_l_cr, valid_r_cr = measure_curvature_in_meters(ym_per_pix,xm_per_pix,ploty,lfit_meters,rfit_meters)\n",
    "    \n",
    "    #update the line buffer with new data                                                                                                                                    \n",
    "    left.setLatestFitx(valid_l_fit_x)\n",
    "    left.setLatestFit(valid_left_fit)\n",
    "    left.setCurvature(valid_l_cr)\n",
    "    right.setLatestFitx(valid_r_fit_x)\n",
    "    right.setLatestFit(valid_right_fit)\n",
    "    right.setCurvature(valid_r_cr)    \n",
    "    LinesDeque.append((left,right))\n",
    "                                                                                                                                          \n",
    "    # prepare visualization\n",
    "    warpedGray = cv2.cvtColor(warped,cv2.COLOR_RGB2GRAY)\n",
    "    #use valid lines \n",
    "    driving_space = color_driving_space(warpedGray,valid_l_fit_x, valid_r_fit_x, ploty)\n",
    "    img_size = (driving_space.shape[1], driving_space.shape[0])\n",
    "                                                                                                                                          \n",
    "    #warp the image back into the original image space           \n",
    "    unwarped = cv2.warpPerspective(driving_space,invM,img_size,flags=cv2.INTER_LINEAR)    \n",
    "    #merge driving space into original image\n",
    "    procesed_image = cv2.addWeighted(image, 1, unwarped, 0.3, 0)\n",
    "    #add tags to the image \n",
    "    add_tags(procesed_image,valid_l_cr,valid_r_cr,valid_l_fit_x[-1], valid_r_fit_x[-1],xm_per_pix,testing=False)\n",
    "    \n",
    "    return procesed_image\n",
    "\n",
    "\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing:  pipeline on warped test images\n",
    "\n",
    "please uncomment the last line to visualize the images (the amount of output images depends on the folder you choose for testing. If no folder is given as input (folder_to_test = '') then the default folder is \"test_images\" (where I have added some additional images with respect to the original 6). Other possible folders are \"project_video\",\"straight_lines\" and \"challenge_video\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeup_test_pipeline_with_images(folder=''):\n",
    "    '''\n",
    "    method: test  several images. This method is used for testing \n",
    "    and for the project writeup showing the output of the lane \n",
    "    finding approaches on some key images for this project    \n",
    "    '''    \n",
    "    \n",
    "    root_path = 'test_images/'\n",
    "    path = root_path + folder\n",
    "\n",
    "    full_path = path + '*.jpg'\n",
    "    \n",
    "    images = glob.glob(full_path)    \n",
    "\n",
    "    for im in images:\n",
    "\n",
    "        imNum = im.split(path)[1].split('.jpg')[0]\n",
    "\n",
    "        image = plt.imread(im)\n",
    "        # Define conversions in x and y from pixels space to meters\n",
    "        ym_per_pix = 15/720 # meters per pixel in y dimension based on visual observation\n",
    "        xm_per_pix = 3.7/600 # meters per pixel in x dimension based on visual observation\n",
    "        \n",
    "        warped,M,invM,src,dst = undistort_and_warp(image,challenge=True)\n",
    "        binary_or_img,bin_sobelx,bin_s_channel,bin_r_channel,bin_sobel_mag = colors_and_gradients(warped)\n",
    "\n",
    "        #lineImg1,left_fitx, right_fitx, ploty,left_fit, right_fit = lane_search_around_poly(binary_or_img,left_fit_prev,right_fit_prev)\n",
    "        find_lanes = lane_search_with_windows(binary_warped,xm_per_pix,ym_per_pix,bottom_half=False)\n",
    "\n",
    "        lineImg1 = find_lanes['out_img']\n",
    "        valid_l_fit_x = find_lanes['left_fitx']\n",
    "        valid_r_fit_x = find_lanes['right_fitx']\n",
    "        ploty = find_lanes['ploty']\n",
    "        valid_left_fit = find_lanes['left_fit']\n",
    "        valid_right_fit = find_lanes['right_fit']\n",
    "        lfit_meters = find_lanes['left_fit_meters']\n",
    "        rfit_meters = find_lanes['right_fit_meters']\n",
    "        \n",
    "\n",
    "        # Calculate the radius of curvature in meters for both lane lines\n",
    "        valid_l_cr, valid_r_cr = measure_curvature_in_meters(ym_per_pix,xm_per_pix,ploty,lfit_meters,rfit_meters)    \n",
    "\n",
    "        warpedGray = cv2.cvtColor(warped,cv2.COLOR_RGB2GRAY)\n",
    "        #use valid lines \n",
    "        driving_space = color_driving_space(warpedGray,valid_l_fit_x, valid_r_fit_x, ploty)\n",
    "        img_size = (driving_space.shape[1], driving_space.shape[0])\n",
    "        unwarped = cv2.warpPerspective(driving_space,invM,img_size,flags=cv2.INTER_LINEAR)    \n",
    "        merged = cv2.addWeighted(image, 1, unwarped, 0.3, 0)\n",
    "        add_tags(merged,valid_l_cr,valid_r_cr,valid_l_fit_x[-1], valid_r_fit_x[-1],xm_per_pix,testing=False)    \n",
    "\n",
    "        \n",
    "\n",
    "        f, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2, 2, figsize=(18, 7))\n",
    "        f.tight_layout()\n",
    "        ax1.imshow(image)\n",
    "        ax1.set_title('Original image {}'.format(imNum),  fontsize=10)\n",
    "        ax2.imshow(merged)\n",
    "        ax2.set_title('Tagged image', fontsize=10)\n",
    "        ax3.imshow(warped)\n",
    "        #cv2.imwrite(\"test%s.jpg\" % imNum, warped)\n",
    "        ax3.set_title('Warped result test image {}'.format(imNum),  fontsize=10)\n",
    "        plt.plot(left_fitx, ploty, color='yellow')\n",
    "        plt.plot(right_fitx, ploty, color='yellow')\n",
    "        ax4.imshow(lineImg1)\n",
    "        ax4.set_title('Detected lanes with\\n corresponding polynomial drawn', fontsize=10)\n",
    "        plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.05,wspace = None,hspace = 0.3)\n",
    "               \n",
    "        #plt.imsave('output_images/colored_driving_space.jpg',merged )\n",
    "        #f.savefig('output_images/writeup_lane_boundaries.jpg')\n",
    "        \n",
    "    \n",
    "folder_to_test = ''\n",
    "folder_to_test  = 'project_video/test'\n",
    "#folder_to_test ='straight_lines/test'\n",
    "#folder_to_test ='challenge_video/test'\n",
    "#writeup_test_pipeline_with_images(folder_to_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing project video\n",
    "run this line to process the project video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   0%|          | 2/1260 [00:00<01:40, 12.47it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video project_video_processed.mp4.\n",
      "Moviepy - Writing video project_video_processed.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready project_video_processed.mp4\n",
      "CPU times: user 5min 7s, sys: 45.6 s, total: 5min 53s\n",
      "Wall time: 3min 26s\n"
     ]
    }
   ],
   "source": [
    "#define buffer size for smoothing\n",
    "max_queue_size = 5\n",
    "LinesDeque = collections.deque(maxlen=max_queue_size)\n",
    "def process_project_video(subclip=False,begin=0,end=5):\n",
    "    output='project_video_processed.mp4'\n",
    "    clip1 = VideoFileClip('project_video.mp4')\n",
    "    if (subclip):\n",
    "        clip = clip1.fl_image(process_frame).subclip(begin,end)\n",
    "    else:\n",
    "        clip = clip1.fl_image(process_frame)\n",
    "    %time clip.write_videofile(output, audio=False)\n",
    "\n",
    "#process_project_video(subclip=True,begin=40,end=42)\n",
    "process_project_video(subclip=False,begin=0,end=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"960\" height=\"540\" controls>\n",
       "  <source src='project_video_processed.mp4'>\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src='project_video_processed.mp4'>\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process challenging video\n",
    "please uncomment the last line to process the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:  81%|████████  | 391/485 [04:51<00:13,  6.75it/s, now=None]\n",
      "t:   0%|          | 0/485 [00:00<?, ?it/s, now=None]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video challenge_video_processed.mp4.\n",
      "Moviepy - Writing video challenge_video_processed.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "t:   0%|          | 2/485 [00:00<00:42, 11.31it/s, now=None]\u001b[A\n",
      "t:   1%|          | 4/485 [00:00<01:14,  6.45it/s, now=None]\u001b[A\n",
      "t:   1%|          | 5/485 [00:00<01:45,  4.57it/s, now=None]\u001b[A\n",
      "t:   1%|          | 6/485 [00:01<01:50,  4.33it/s, now=None]\u001b[A\n",
      "t:   1%|▏         | 7/485 [00:01<02:05,  3.80it/s, now=None]\u001b[A\n",
      "t:   2%|▏         | 8/485 [00:01<02:05,  3.81it/s, now=None]\u001b[A\n",
      "t:   2%|▏         | 9/485 [00:02<02:04,  3.81it/s, now=None]\u001b[A\n",
      "t:   2%|▏         | 10/485 [00:02<02:08,  3.70it/s, now=None]\u001b[A\n",
      "t:   2%|▏         | 11/485 [00:02<01:56,  4.05it/s, now=None]\u001b[A\n",
      "t:   2%|▏         | 12/485 [00:02<01:57,  4.02it/s, now=None]\u001b[A\n",
      "t:   3%|▎         | 13/485 [00:03<01:50,  4.29it/s, now=None]\u001b[A\n",
      "t:   3%|▎         | 14/485 [00:03<01:40,  4.68it/s, now=None]\u001b[A\n",
      "t:   3%|▎         | 15/485 [00:03<01:34,  4.98it/s, now=None]\u001b[A\n",
      "t:   3%|▎         | 16/485 [00:03<01:30,  5.16it/s, now=None]\u001b[A\n",
      "t:   4%|▎         | 17/485 [00:03<01:23,  5.62it/s, now=None]\u001b[A\n",
      "t:   4%|▎         | 18/485 [00:03<01:19,  5.91it/s, now=None]\u001b[A\n",
      "t:   4%|▍         | 19/485 [00:03<01:16,  6.10it/s, now=None]\u001b[A\n",
      "t:   4%|▍         | 20/485 [00:04<01:13,  6.30it/s, now=None]\u001b[A\n",
      "t:   4%|▍         | 21/485 [00:04<01:11,  6.46it/s, now=None]\u001b[A\n",
      "t:   5%|▍         | 22/485 [00:04<01:14,  6.22it/s, now=None]\u001b[A\n",
      "t:   5%|▍         | 23/485 [00:04<01:12,  6.37it/s, now=None]\u001b[A\n",
      "t:   5%|▍         | 24/485 [00:04<01:16,  5.99it/s, now=None]\u001b[A\n",
      "t:   5%|▌         | 25/485 [00:04<01:24,  5.43it/s, now=None]\u001b[A\n",
      "t:   5%|▌         | 26/485 [00:05<01:21,  5.61it/s, now=None]\u001b[A\n",
      "t:   6%|▌         | 27/485 [00:05<01:21,  5.59it/s, now=None]\u001b[A\n",
      "t:   6%|▌         | 28/485 [00:05<01:17,  5.91it/s, now=None]\u001b[A\n",
      "t:   6%|▌         | 29/485 [00:05<01:17,  5.86it/s, now=None]\u001b[A\n",
      "t:   6%|▌         | 30/485 [00:05<01:15,  5.99it/s, now=None]\u001b[A\n",
      "t:   6%|▋         | 31/485 [00:05<01:17,  5.87it/s, now=None]\u001b[A\n",
      "t:   7%|▋         | 32/485 [00:06<01:18,  5.74it/s, now=None]\u001b[A\n",
      "t:   7%|▋         | 33/485 [00:06<01:22,  5.49it/s, now=None]\u001b[A\n",
      "t:   7%|▋         | 34/485 [00:06<01:22,  5.49it/s, now=None]\u001b[A\n",
      "t:   7%|▋         | 35/485 [00:06<01:22,  5.47it/s, now=None]\u001b[A\n",
      "t:   7%|▋         | 36/485 [00:06<01:20,  5.56it/s, now=None]\u001b[A\n",
      "t:   8%|▊         | 37/485 [00:07<01:20,  5.56it/s, now=None]\u001b[A\n",
      "t:   8%|▊         | 38/485 [00:07<01:23,  5.37it/s, now=None]\u001b[A\n",
      "t:   8%|▊         | 39/485 [00:07<01:22,  5.38it/s, now=None]\u001b[A\n",
      "t:   8%|▊         | 40/485 [00:07<01:35,  4.67it/s, now=None]\u001b[A\n",
      "t:   8%|▊         | 41/485 [00:07<01:31,  4.86it/s, now=None]\u001b[A\n",
      "t:   9%|▊         | 42/485 [00:08<01:30,  4.90it/s, now=None]\u001b[A\n",
      "t:   9%|▉         | 43/485 [00:08<01:32,  4.78it/s, now=None]\u001b[A\n",
      "t:   9%|▉         | 44/485 [00:08<01:32,  4.78it/s, now=None]\u001b[A\n",
      "t:   9%|▉         | 45/485 [00:08<01:40,  4.38it/s, now=None]\u001b[A\n",
      "t:   9%|▉         | 46/485 [00:09<01:30,  4.87it/s, now=None]\u001b[A\n",
      "t:  10%|▉         | 47/485 [00:09<01:26,  5.06it/s, now=None]\u001b[A\n",
      "t:  10%|▉         | 48/485 [00:09<01:35,  4.58it/s, now=None]\u001b[A\n",
      "t:  10%|█         | 49/485 [00:09<01:35,  4.56it/s, now=None]\u001b[A\n",
      "t:  10%|█         | 50/485 [00:09<01:32,  4.70it/s, now=None]\u001b[A\n",
      "t:  11%|█         | 51/485 [00:10<01:31,  4.76it/s, now=None]\u001b[A\n",
      "t:  11%|█         | 52/485 [00:10<01:38,  4.40it/s, now=None]\u001b[A\n",
      "t:  11%|█         | 53/485 [00:10<01:33,  4.60it/s, now=None]\u001b[A\n",
      "t:  11%|█         | 54/485 [00:10<01:28,  4.89it/s, now=None]\u001b[A\n",
      "t:  11%|█▏        | 55/485 [00:10<01:23,  5.12it/s, now=None]\u001b[A\n",
      "t:  12%|█▏        | 56/485 [00:11<01:26,  4.97it/s, now=None]\u001b[A\n",
      "t:  12%|█▏        | 57/485 [00:11<01:30,  4.74it/s, now=None]\u001b[A\n",
      "t:  12%|█▏        | 58/485 [00:11<01:28,  4.81it/s, now=None]\u001b[A\n",
      "t:  12%|█▏        | 59/485 [00:11<01:28,  4.82it/s, now=None]\u001b[A\n",
      "t:  12%|█▏        | 60/485 [00:12<01:37,  4.37it/s, now=None]\u001b[A\n",
      "t:  13%|█▎        | 61/485 [00:12<01:52,  3.78it/s, now=None]\u001b[A\n",
      "t:  13%|█▎        | 62/485 [00:12<01:47,  3.93it/s, now=None]\u001b[A\n",
      "t:  13%|█▎        | 63/485 [00:12<01:38,  4.28it/s, now=None]\u001b[A\n",
      "t:  13%|█▎        | 64/485 [00:12<01:31,  4.60it/s, now=None]\u001b[A\n",
      "t:  13%|█▎        | 65/485 [00:13<01:31,  4.59it/s, now=None]\u001b[A\n",
      "t:  14%|█▎        | 66/485 [00:13<01:28,  4.72it/s, now=None]\u001b[A\n",
      "t:  14%|█▍        | 67/485 [00:13<01:25,  4.88it/s, now=None]\u001b[A\n",
      "t:  14%|█▍        | 68/485 [00:13<01:26,  4.83it/s, now=None]\u001b[A\n",
      "t:  14%|█▍        | 69/485 [00:14<01:29,  4.66it/s, now=None]\u001b[A\n",
      "t:  14%|█▍        | 70/485 [00:14<01:31,  4.53it/s, now=None]\u001b[A\n",
      "t:  15%|█▍        | 71/485 [00:14<01:31,  4.51it/s, now=None]\u001b[A\n",
      "t:  15%|█▍        | 72/485 [00:14<01:27,  4.70it/s, now=None]\u001b[A\n",
      "t:  15%|█▌        | 73/485 [00:14<01:27,  4.71it/s, now=None]\u001b[A\n",
      "t:  15%|█▌        | 74/485 [00:15<01:29,  4.58it/s, now=None]\u001b[A\n",
      "t:  15%|█▌        | 75/485 [00:15<01:26,  4.76it/s, now=None]\u001b[A\n",
      "t:  16%|█▌        | 76/485 [00:15<01:29,  4.59it/s, now=None]\u001b[A\n",
      "t:  16%|█▌        | 77/485 [00:15<01:25,  4.76it/s, now=None]\u001b[A\n",
      "t:  16%|█▌        | 78/485 [00:15<01:25,  4.78it/s, now=None]\u001b[A\n",
      "t:  16%|█▋        | 79/485 [00:16<01:21,  4.96it/s, now=None]\u001b[A\n",
      "t:  16%|█▋        | 80/485 [00:16<01:22,  4.93it/s, now=None]\u001b[A\n",
      "t:  17%|█▋        | 81/485 [00:16<01:24,  4.77it/s, now=None]\u001b[A\n",
      "t:  17%|█▋        | 82/485 [00:16<01:21,  4.96it/s, now=None]\u001b[A\n",
      "t:  17%|█▋        | 83/485 [00:16<01:24,  4.73it/s, now=None]\u001b[A\n",
      "t:  17%|█▋        | 84/485 [00:17<01:24,  4.75it/s, now=None]\u001b[A\n",
      "t:  18%|█▊        | 85/485 [00:17<01:40,  3.96it/s, now=None]\u001b[A\n",
      "t:  18%|█▊        | 86/485 [00:17<01:50,  3.61it/s, now=None]\u001b[A\n",
      "t:  18%|█▊        | 87/485 [00:18<02:10,  3.06it/s, now=None]\u001b[A\n",
      "t:  18%|█▊        | 88/485 [00:18<01:59,  3.33it/s, now=None]\u001b[A\n",
      "t:  18%|█▊        | 89/485 [00:18<01:47,  3.70it/s, now=None]\u001b[A\n",
      "t:  19%|█▊        | 90/485 [00:18<01:34,  4.19it/s, now=None]\u001b[A\n",
      "t:  19%|█▉        | 91/485 [00:19<01:24,  4.68it/s, now=None]\u001b[A\n",
      "t:  19%|█▉        | 92/485 [00:19<01:14,  5.26it/s, now=None]\u001b[A\n",
      "t:  19%|█▉        | 93/485 [00:19<01:10,  5.58it/s, now=None]\u001b[A\n",
      "t:  19%|█▉        | 94/485 [00:19<01:04,  6.05it/s, now=None]\u001b[A\n",
      "t:  20%|█▉        | 95/485 [00:19<01:00,  6.44it/s, now=None]\u001b[A\n",
      "t:  20%|█▉        | 96/485 [00:19<01:02,  6.21it/s, now=None]\u001b[A\n",
      "t:  20%|██        | 97/485 [00:20<01:18,  4.95it/s, now=None]\u001b[A\n",
      "t:  20%|██        | 98/485 [00:20<01:13,  5.28it/s, now=None]\u001b[A\n",
      "t:  20%|██        | 99/485 [00:20<01:07,  5.73it/s, now=None]\u001b[A\n",
      "t:  21%|██        | 100/485 [00:20<01:02,  6.18it/s, now=None]\u001b[A\n",
      "t:  21%|██        | 101/485 [00:20<01:02,  6.10it/s, now=None]\u001b[A\n",
      "t:  21%|██        | 102/485 [00:20<01:11,  5.32it/s, now=None]\u001b[A\n",
      "t:  21%|██        | 103/485 [00:21<01:08,  5.60it/s, now=None]\u001b[A\n",
      "t:  21%|██▏       | 104/485 [00:21<01:03,  5.99it/s, now=None]\u001b[A\n",
      "t:  22%|██▏       | 105/485 [00:21<01:03,  5.97it/s, now=None]\u001b[A\n",
      "t:  22%|██▏       | 106/485 [00:21<01:02,  6.03it/s, now=None]\u001b[A\n",
      "t:  22%|██▏       | 107/485 [00:21<00:59,  6.33it/s, now=None]\u001b[A\n",
      "t:  22%|██▏       | 108/485 [00:21<00:58,  6.50it/s, now=None]\u001b[A\n",
      "t:  22%|██▏       | 109/485 [00:22<00:59,  6.35it/s, now=None]\u001b[A\n",
      "t:  23%|██▎       | 110/485 [00:22<00:58,  6.40it/s, now=None]\u001b[A\n",
      "t:  23%|██▎       | 111/485 [00:22<00:57,  6.52it/s, now=None]\u001b[A\n",
      "t:  23%|██▎       | 112/485 [00:22<00:58,  6.36it/s, now=None]\u001b[A\n",
      "t:  23%|██▎       | 113/485 [00:22<00:59,  6.21it/s, now=None]\u001b[A\n",
      "t:  24%|██▎       | 114/485 [00:22<00:59,  6.25it/s, now=None]\u001b[A\n",
      "t:  24%|██▎       | 115/485 [00:23<01:03,  5.80it/s, now=None]\u001b[A\n",
      "t:  24%|██▍       | 116/485 [00:23<01:03,  5.80it/s, now=None]\u001b[A\n",
      "t:  24%|██▍       | 117/485 [00:23<01:10,  5.20it/s, now=None]\u001b[A\n",
      "t:  24%|██▍       | 118/485 [00:23<01:13,  4.97it/s, now=None]\u001b[A\n",
      "t:  25%|██▍       | 119/485 [00:24<01:44,  3.50it/s, now=None]\u001b[A\n",
      "t:  25%|██▍       | 120/485 [00:24<01:39,  3.66it/s, now=None]\u001b[A\n",
      "t:  25%|██▍       | 121/485 [00:24<01:57,  3.09it/s, now=None]\u001b[A\n",
      "t:  25%|██▌       | 122/485 [00:25<01:52,  3.23it/s, now=None]\u001b[A\n",
      "t:  25%|██▌       | 123/485 [00:25<01:48,  3.34it/s, now=None]\u001b[A\n",
      "t:  26%|██▌       | 124/485 [00:25<01:44,  3.44it/s, now=None]\u001b[A\n",
      "t:  26%|██▌       | 125/485 [00:25<01:40,  3.58it/s, now=None]\u001b[A\n",
      "t:  26%|██▌       | 126/485 [00:26<01:30,  3.95it/s, now=None]\u001b[A\n",
      "t:  26%|██▌       | 127/485 [00:26<01:58,  3.02it/s, now=None]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:  26%|██▋       | 128/485 [00:26<01:52,  3.17it/s, now=None]\u001b[A\n",
      "t:  27%|██▋       | 129/485 [00:27<02:07,  2.79it/s, now=None]\u001b[A\n",
      "t:  27%|██▋       | 130/485 [00:27<02:36,  2.28it/s, now=None]\u001b[A\n",
      "t:  27%|██▋       | 131/485 [00:28<02:24,  2.45it/s, now=None]\u001b[A\n",
      "t:  27%|██▋       | 132/485 [00:28<02:40,  2.20it/s, now=None]\u001b[A\n",
      "t:  27%|██▋       | 133/485 [00:29<02:41,  2.18it/s, now=None]\u001b[A\n",
      "t:  28%|██▊       | 134/485 [00:29<02:43,  2.14it/s, now=None]\u001b[A\n",
      "t:  28%|██▊       | 135/485 [00:30<02:27,  2.37it/s, now=None]\u001b[A\n",
      "t:  28%|██▊       | 136/485 [00:30<02:16,  2.55it/s, now=None]\u001b[A\n",
      "t:  28%|██▊       | 137/485 [00:30<02:10,  2.67it/s, now=None]\u001b[A\n",
      "t:  28%|██▊       | 138/485 [00:31<01:58,  2.93it/s, now=None]\u001b[A\n",
      "t:  29%|██▊       | 139/485 [00:31<01:43,  3.36it/s, now=None]\u001b[A\n",
      "t:  29%|██▉       | 140/485 [00:31<01:30,  3.82it/s, now=None]\u001b[A\n",
      "t:  29%|██▉       | 141/485 [00:31<01:19,  4.33it/s, now=None]\u001b[A\n",
      "t:  29%|██▉       | 142/485 [00:31<01:18,  4.36it/s, now=None]\u001b[A\n",
      "t:  29%|██▉       | 143/485 [00:32<01:34,  3.61it/s, now=None]\u001b[A\n",
      "t:  30%|██▉       | 144/485 [00:32<01:27,  3.89it/s, now=None]\u001b[A\n",
      "t:  30%|██▉       | 145/485 [00:32<01:22,  4.11it/s, now=None]\u001b[A\n",
      "t:  30%|███       | 146/485 [00:32<01:26,  3.93it/s, now=None]\u001b[A\n",
      "t:  30%|███       | 147/485 [00:33<01:24,  4.01it/s, now=None]\u001b[A\n",
      "t:  31%|███       | 148/485 [00:33<01:17,  4.34it/s, now=None]\u001b[A\n",
      "t:  31%|███       | 149/485 [00:33<01:13,  4.58it/s, now=None]\u001b[A\n",
      "t:  31%|███       | 150/485 [00:33<01:14,  4.49it/s, now=None]\u001b[A\n",
      "t:  31%|███       | 151/485 [00:33<01:08,  4.87it/s, now=None]\u001b[A\n",
      "t:  31%|███▏      | 152/485 [00:34<01:05,  5.07it/s, now=None]\u001b[A\n",
      "t:  32%|███▏      | 153/485 [00:34<01:00,  5.51it/s, now=None]\u001b[A\n",
      "t:  32%|███▏      | 154/485 [00:34<00:59,  5.60it/s, now=None]\u001b[A\n",
      "t:  32%|███▏      | 155/485 [00:34<00:58,  5.67it/s, now=None]\u001b[A\n",
      "t:  32%|███▏      | 156/485 [00:34<00:58,  5.65it/s, now=None]\u001b[A\n",
      "t:  32%|███▏      | 157/485 [00:34<01:00,  5.46it/s, now=None]\u001b[A\n",
      "t:  33%|███▎      | 158/485 [00:35<00:56,  5.75it/s, now=None]\u001b[A\n",
      "t:  33%|███▎      | 159/485 [00:35<00:58,  5.59it/s, now=None]\u001b[A\n",
      "t:  33%|███▎      | 160/485 [00:35<00:56,  5.75it/s, now=None]\u001b[A\n",
      "t:  33%|███▎      | 161/485 [00:35<00:54,  5.90it/s, now=None]\u001b[A\n",
      "t:  33%|███▎      | 162/485 [00:35<00:55,  5.83it/s, now=None]\u001b[A\n",
      "t:  34%|███▎      | 163/485 [00:35<00:58,  5.48it/s, now=None]\u001b[A\n",
      "t:  34%|███▍      | 164/485 [00:36<00:57,  5.60it/s, now=None]\u001b[A\n",
      "t:  34%|███▍      | 165/485 [00:36<00:53,  5.96it/s, now=None]\u001b[A\n",
      "t:  34%|███▍      | 166/485 [00:36<00:51,  6.22it/s, now=None]\u001b[A\n",
      "t:  34%|███▍      | 167/485 [00:36<00:53,  5.89it/s, now=None]\u001b[A\n",
      "t:  35%|███▍      | 168/485 [00:36<00:55,  5.68it/s, now=None]\u001b[A\n",
      "t:  35%|███▍      | 169/485 [00:36<00:53,  5.94it/s, now=None]\u001b[A\n",
      "t:  35%|███▌      | 170/485 [00:37<00:53,  5.91it/s, now=None]\u001b[A\n",
      "t:  35%|███▌      | 171/485 [00:37<00:53,  5.88it/s, now=None]\u001b[A\n",
      "t:  35%|███▌      | 172/485 [00:38<02:04,  2.52it/s, now=None]\u001b[A\n",
      "t:  36%|███▌      | 173/485 [00:38<01:58,  2.63it/s, now=None]\u001b[A\n",
      "t:  36%|███▌      | 174/485 [00:39<02:08,  2.43it/s, now=None]\u001b[A\n",
      "t:  36%|███▌      | 175/485 [00:39<02:03,  2.50it/s, now=None]\u001b[A\n",
      "t:  36%|███▋      | 176/485 [00:40<02:36,  1.98it/s, now=None]\u001b[A\n",
      "t:  36%|███▋      | 177/485 [00:40<02:47,  1.84it/s, now=None]\u001b[A\n",
      "t:  37%|███▋      | 178/485 [00:41<02:44,  1.87it/s, now=None]\u001b[A\n",
      "t:  37%|███▋      | 179/485 [00:41<02:14,  2.27it/s, now=None]\u001b[A\n",
      "t:  37%|███▋      | 180/485 [00:41<01:51,  2.74it/s, now=None]\u001b[A\n",
      "t:  37%|███▋      | 181/485 [00:41<01:38,  3.08it/s, now=None]\u001b[A\n",
      "t:  38%|███▊      | 182/485 [00:42<01:45,  2.87it/s, now=None]\u001b[A\n",
      "t:  38%|███▊      | 183/485 [00:42<01:37,  3.09it/s, now=None]\u001b[A\n",
      "t:  38%|███▊      | 184/485 [00:43<01:53,  2.65it/s, now=None]\u001b[A\n",
      "t:  38%|███▊      | 185/485 [00:43<01:46,  2.83it/s, now=None]\u001b[A\n",
      "t:  38%|███▊      | 186/485 [00:43<01:37,  3.05it/s, now=None]\u001b[A\n",
      "t:  39%|███▊      | 187/485 [00:44<01:45,  2.82it/s, now=None]\u001b[A\n",
      "t:  39%|███▉      | 188/485 [00:44<01:38,  3.01it/s, now=None]\u001b[A\n",
      "t:  39%|███▉      | 189/485 [00:44<01:31,  3.22it/s, now=None]\u001b[A\n",
      "t:  39%|███▉      | 190/485 [00:44<01:27,  3.38it/s, now=None]\u001b[A\n",
      "t:  39%|███▉      | 191/485 [00:45<01:20,  3.63it/s, now=None]\u001b[A\n",
      "t:  40%|███▉      | 192/485 [00:45<01:13,  3.96it/s, now=None]\u001b[A\n",
      "t:  40%|███▉      | 193/485 [00:45<01:08,  4.24it/s, now=None]\u001b[A\n",
      "t:  40%|████      | 194/485 [00:45<01:05,  4.47it/s, now=None]\u001b[A\n",
      "t:  40%|████      | 195/485 [00:45<00:59,  4.85it/s, now=None]\u001b[A\n",
      "t:  40%|████      | 196/485 [00:46<00:59,  4.89it/s, now=None]\u001b[A\n",
      "t:  41%|████      | 197/485 [00:46<01:23,  3.46it/s, now=None]\u001b[A\n",
      "t:  41%|████      | 198/485 [00:46<01:21,  3.51it/s, now=None]\u001b[A\n",
      "t:  41%|████      | 199/485 [00:47<01:12,  3.95it/s, now=None]\u001b[A\n",
      "t:  41%|████      | 200/485 [00:47<01:08,  4.13it/s, now=None]\u001b[A\n",
      "t:  41%|████▏     | 201/485 [00:47<01:08,  4.16it/s, now=None]\u001b[A\n",
      "t:  42%|████▏     | 202/485 [00:47<01:05,  4.32it/s, now=None]\u001b[A\n",
      "t:  42%|████▏     | 203/485 [00:47<01:00,  4.63it/s, now=None]\u001b[A\n",
      "t:  42%|████▏     | 204/485 [00:48<00:55,  5.08it/s, now=None]\u001b[A\n",
      "t:  42%|████▏     | 205/485 [00:48<00:51,  5.48it/s, now=None]\u001b[A\n",
      "t:  42%|████▏     | 206/485 [00:48<00:50,  5.56it/s, now=None]\u001b[A\n",
      "t:  43%|████▎     | 207/485 [00:48<00:50,  5.45it/s, now=None]\u001b[A\n",
      "t:  43%|████▎     | 208/485 [00:48<00:49,  5.59it/s, now=None]\u001b[A\n",
      "t:  43%|████▎     | 209/485 [00:48<00:47,  5.80it/s, now=None]\u001b[A\n",
      "t:  43%|████▎     | 210/485 [00:49<00:48,  5.63it/s, now=None]\u001b[A\n",
      "t:  44%|████▎     | 211/485 [00:49<00:47,  5.80it/s, now=None]\u001b[A\n",
      "t:  44%|████▎     | 212/485 [00:49<00:48,  5.69it/s, now=None]\u001b[A\n",
      "t:  44%|████▍     | 213/485 [00:49<00:44,  6.09it/s, now=None]\u001b[A\n",
      "t:  44%|████▍     | 214/485 [00:49<00:44,  6.04it/s, now=None]\u001b[A\n",
      "t:  44%|████▍     | 215/485 [00:49<00:42,  6.32it/s, now=None]\u001b[A\n",
      "t:  45%|████▍     | 216/485 [00:50<00:41,  6.54it/s, now=None]\u001b[A\n",
      "t:  45%|████▍     | 217/485 [00:50<00:43,  6.23it/s, now=None]\u001b[A\n",
      "t:  45%|████▍     | 218/485 [00:50<00:44,  5.98it/s, now=None]\u001b[A\n",
      "t:  45%|████▌     | 219/485 [00:50<00:42,  6.22it/s, now=None]\u001b[A\n",
      "t:  45%|████▌     | 220/485 [00:50<00:40,  6.53it/s, now=None]\u001b[A\n",
      "t:  46%|████▌     | 221/485 [00:50<00:40,  6.58it/s, now=None]\u001b[A\n",
      "t:  46%|████▌     | 222/485 [00:50<00:42,  6.26it/s, now=None]\u001b[A\n",
      "t:  46%|████▌     | 223/485 [00:51<00:41,  6.26it/s, now=None]\u001b[A\n",
      "t:  46%|████▌     | 224/485 [00:51<00:40,  6.40it/s, now=None]\u001b[A\n",
      "t:  46%|████▋     | 225/485 [00:51<00:39,  6.57it/s, now=None]\u001b[A\n",
      "t:  47%|████▋     | 226/485 [00:51<00:40,  6.34it/s, now=None]\u001b[A\n",
      "t:  47%|████▋     | 227/485 [00:51<00:42,  6.09it/s, now=None]\u001b[A\n",
      "t:  47%|████▋     | 228/485 [00:51<00:40,  6.39it/s, now=None]\u001b[A\n",
      "t:  47%|████▋     | 229/485 [00:52<00:39,  6.53it/s, now=None]\u001b[A\n",
      "t:  47%|████▋     | 230/485 [00:52<00:40,  6.30it/s, now=None]\u001b[A\n",
      "t:  48%|████▊     | 231/485 [00:52<00:40,  6.33it/s, now=None]\u001b[A\n",
      "t:  48%|████▊     | 232/485 [00:52<00:41,  6.17it/s, now=None]\u001b[A\n",
      "t:  48%|████▊     | 233/485 [00:52<00:40,  6.26it/s, now=None]\u001b[A\n",
      "t:  48%|████▊     | 234/485 [00:52<00:41,  5.99it/s, now=None]\u001b[A\n",
      "t:  48%|████▊     | 235/485 [00:53<00:41,  6.05it/s, now=None]\u001b[A\n",
      "t:  49%|████▊     | 236/485 [00:53<00:40,  6.17it/s, now=None]\u001b[A\n",
      "t:  49%|████▉     | 237/485 [00:53<00:39,  6.25it/s, now=None]\u001b[A\n",
      "t:  49%|████▉     | 238/485 [00:53<00:39,  6.30it/s, now=None]\u001b[A\n",
      "t:  49%|████▉     | 239/485 [00:53<00:37,  6.56it/s, now=None]\u001b[A\n",
      "t:  49%|████▉     | 240/485 [00:53<00:36,  6.72it/s, now=None]\u001b[A\n",
      "t:  50%|████▉     | 241/485 [00:53<00:36,  6.74it/s, now=None]\u001b[A\n",
      "t:  50%|████▉     | 242/485 [00:54<00:39,  6.16it/s, now=None]\u001b[A\n",
      "t:  50%|█████     | 243/485 [00:54<00:38,  6.23it/s, now=None]\u001b[A\n",
      "t:  50%|█████     | 244/485 [00:54<00:38,  6.27it/s, now=None]\u001b[A\n",
      "t:  51%|█████     | 245/485 [00:54<00:38,  6.19it/s, now=None]\u001b[A\n",
      "t:  51%|█████     | 246/485 [00:54<00:39,  6.11it/s, now=None]\u001b[A\n",
      "t:  51%|█████     | 247/485 [00:54<00:39,  5.97it/s, now=None]\u001b[A\n",
      "t:  51%|█████     | 248/485 [00:55<00:38,  6.12it/s, now=None]\u001b[A\n",
      "t:  51%|█████▏    | 249/485 [00:55<00:38,  6.10it/s, now=None]\u001b[A\n",
      "t:  52%|█████▏    | 250/485 [00:55<00:39,  5.97it/s, now=None]\u001b[A\n",
      "t:  52%|█████▏    | 251/485 [00:55<00:38,  6.12it/s, now=None]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:  52%|█████▏    | 252/485 [00:55<00:37,  6.17it/s, now=None]\u001b[A\n",
      "t:  52%|█████▏    | 253/485 [00:55<00:35,  6.51it/s, now=None]\u001b[A\n",
      "t:  52%|█████▏    | 254/485 [00:56<00:35,  6.46it/s, now=None]\u001b[A\n",
      "t:  53%|█████▎    | 255/485 [00:56<00:34,  6.67it/s, now=None]\u001b[A\n",
      "t:  53%|█████▎    | 256/485 [00:56<00:33,  6.93it/s, now=None]\u001b[A\n",
      "t:  53%|█████▎    | 257/485 [00:56<00:33,  6.84it/s, now=None]\u001b[A\n",
      "t:  53%|█████▎    | 258/485 [00:56<00:33,  6.74it/s, now=None]\u001b[A\n",
      "t:  53%|█████▎    | 259/485 [00:56<00:33,  6.84it/s, now=None]\u001b[A\n",
      "t:  54%|█████▎    | 260/485 [00:56<00:32,  6.99it/s, now=None]\u001b[A\n",
      "t:  54%|█████▍    | 261/485 [00:57<00:31,  7.09it/s, now=None]\u001b[A\n",
      "t:  54%|█████▍    | 262/485 [00:57<00:33,  6.76it/s, now=None]\u001b[A\n",
      "t:  54%|█████▍    | 263/485 [00:57<00:32,  6.78it/s, now=None]\u001b[A\n",
      "t:  54%|█████▍    | 264/485 [00:57<00:34,  6.44it/s, now=None]\u001b[A\n",
      "t:  55%|█████▍    | 265/485 [00:57<00:34,  6.42it/s, now=None]\u001b[A\n",
      "t:  55%|█████▍    | 266/485 [00:57<00:34,  6.38it/s, now=None]\u001b[A\n",
      "t:  55%|█████▌    | 267/485 [00:58<00:34,  6.35it/s, now=None]\u001b[A\n",
      "t:  55%|█████▌    | 268/485 [00:58<00:33,  6.43it/s, now=None]\u001b[A\n",
      "t:  55%|█████▌    | 269/485 [00:58<00:32,  6.57it/s, now=None]\u001b[A\n",
      "t:  56%|█████▌    | 270/485 [00:58<00:31,  6.80it/s, now=None]\u001b[A\n",
      "t:  56%|█████▌    | 271/485 [00:58<00:30,  7.04it/s, now=None]\u001b[A\n",
      "t:  56%|█████▌    | 272/485 [00:58<00:32,  6.59it/s, now=None]\u001b[A\n",
      "t:  56%|█████▋    | 273/485 [00:58<00:31,  6.67it/s, now=None]\u001b[A\n",
      "t:  56%|█████▋    | 274/485 [00:59<00:30,  6.88it/s, now=None]\u001b[A\n",
      "t:  57%|█████▋    | 275/485 [00:59<00:29,  7.03it/s, now=None]\u001b[A\n",
      "t:  57%|█████▋    | 276/485 [00:59<00:31,  6.73it/s, now=None]\u001b[A\n",
      "t:  57%|█████▋    | 277/485 [00:59<00:31,  6.53it/s, now=None]\u001b[A\n",
      "t:  57%|█████▋    | 278/485 [00:59<00:30,  6.76it/s, now=None]\u001b[A\n",
      "t:  58%|█████▊    | 279/485 [00:59<00:29,  6.96it/s, now=None]\u001b[A\n",
      "t:  58%|█████▊    | 280/485 [00:59<00:29,  6.86it/s, now=None]\u001b[A\n",
      "t:  58%|█████▊    | 281/485 [01:00<00:29,  6.99it/s, now=None]\u001b[A\n",
      "t:  58%|█████▊    | 282/485 [01:00<00:29,  6.89it/s, now=None]\u001b[A\n",
      "t:  58%|█████▊    | 283/485 [01:00<00:28,  7.07it/s, now=None]\u001b[A\n",
      "t:  59%|█████▊    | 284/485 [01:00<00:29,  6.92it/s, now=None]\u001b[A\n",
      "t:  59%|█████▉    | 285/485 [01:00<00:28,  6.97it/s, now=None]\u001b[A\n",
      "t:  59%|█████▉    | 286/485 [01:00<00:28,  7.07it/s, now=None]\u001b[A\n",
      "t:  59%|█████▉    | 287/485 [01:00<00:28,  6.93it/s, now=None]\u001b[A\n",
      "t:  59%|█████▉    | 288/485 [01:01<00:28,  6.82it/s, now=None]\u001b[A\n",
      "t:  60%|█████▉    | 289/485 [01:01<00:28,  6.90it/s, now=None]\u001b[A\n",
      "t:  60%|█████▉    | 290/485 [01:01<00:27,  7.02it/s, now=None]\u001b[A\n",
      "t:  60%|██████    | 291/485 [01:01<00:27,  7.18it/s, now=None]\u001b[A\n",
      "t:  60%|██████    | 292/485 [01:01<00:28,  6.71it/s, now=None]\u001b[A\n",
      "t:  60%|██████    | 293/485 [01:01<00:28,  6.83it/s, now=None]\u001b[A\n",
      "t:  61%|██████    | 294/485 [01:01<00:29,  6.55it/s, now=None]\u001b[A\n",
      "t:  61%|██████    | 295/485 [01:02<00:28,  6.70it/s, now=None]\u001b[A\n",
      "t:  61%|██████    | 296/485 [01:02<00:27,  6.84it/s, now=None]\u001b[A\n",
      "t:  61%|██████    | 297/485 [01:02<00:27,  6.75it/s, now=None]\u001b[A\n",
      "t:  61%|██████▏   | 298/485 [01:02<00:26,  6.97it/s, now=None]\u001b[A\n",
      "t:  62%|██████▏   | 299/485 [01:02<00:26,  6.98it/s, now=None]\u001b[A\n",
      "t:  62%|██████▏   | 300/485 [01:02<00:26,  6.97it/s, now=None]\u001b[A\n",
      "t:  62%|██████▏   | 301/485 [01:02<00:25,  7.12it/s, now=None]\u001b[A\n",
      "t:  62%|██████▏   | 302/485 [01:03<00:26,  6.88it/s, now=None]\u001b[A\n",
      "t:  62%|██████▏   | 303/485 [01:03<00:26,  6.94it/s, now=None]\u001b[A\n",
      "t:  63%|██████▎   | 304/485 [01:03<00:25,  7.04it/s, now=None]\u001b[A\n",
      "t:  63%|██████▎   | 305/485 [01:03<00:25,  7.14it/s, now=None]\u001b[A\n",
      "t:  63%|██████▎   | 306/485 [01:03<00:25,  7.03it/s, now=None]\u001b[A\n",
      "t:  63%|██████▎   | 307/485 [01:03<00:26,  6.78it/s, now=None]\u001b[A\n",
      "t:  64%|██████▎   | 308/485 [01:03<00:26,  6.63it/s, now=None]\u001b[A\n",
      "t:  64%|██████▎   | 309/485 [01:04<00:25,  6.83it/s, now=None]\u001b[A\n",
      "t:  64%|██████▍   | 310/485 [01:04<00:26,  6.61it/s, now=None]\u001b[A\n",
      "t:  64%|██████▍   | 311/485 [01:04<00:25,  6.78it/s, now=None]\u001b[A\n",
      "t:  64%|██████▍   | 312/485 [01:04<00:25,  6.71it/s, now=None]\u001b[A\n",
      "t:  65%|██████▍   | 313/485 [01:04<00:28,  5.94it/s, now=None]\u001b[A\n",
      "t:  65%|██████▍   | 314/485 [01:05<00:30,  5.53it/s, now=None]\u001b[A\n",
      "t:  65%|██████▍   | 315/485 [01:05<00:30,  5.51it/s, now=None]\u001b[A\n",
      "t:  65%|██████▌   | 316/485 [01:05<00:30,  5.57it/s, now=None]\u001b[A\n",
      "t:  65%|██████▌   | 317/485 [01:05<00:30,  5.47it/s, now=None]\u001b[A\n",
      "t:  66%|██████▌   | 318/485 [01:05<00:31,  5.24it/s, now=None]\u001b[A\n",
      "t:  66%|██████▌   | 319/485 [01:05<00:30,  5.43it/s, now=None]\u001b[A\n",
      "t:  66%|██████▌   | 320/485 [01:06<00:29,  5.54it/s, now=None]\u001b[A\n",
      "t:  66%|██████▌   | 321/485 [01:06<00:29,  5.61it/s, now=None]\u001b[A\n",
      "t:  66%|██████▋   | 322/485 [01:06<00:31,  5.25it/s, now=None]\u001b[A\n",
      "t:  67%|██████▋   | 323/485 [01:06<00:34,  4.76it/s, now=None]\u001b[A\n",
      "t:  67%|██████▋   | 324/485 [01:07<00:37,  4.33it/s, now=None]\u001b[A\n",
      "t:  67%|██████▋   | 325/485 [01:07<00:34,  4.70it/s, now=None]\u001b[A\n",
      "t:  67%|██████▋   | 326/485 [01:07<00:32,  4.86it/s, now=None]\u001b[A\n",
      "t:  67%|██████▋   | 327/485 [01:07<00:32,  4.87it/s, now=None]\u001b[A\n",
      "t:  68%|██████▊   | 328/485 [01:07<00:31,  4.93it/s, now=None]\u001b[A\n",
      "t:  68%|██████▊   | 329/485 [01:07<00:30,  5.15it/s, now=None]\u001b[A\n",
      "t:  68%|██████▊   | 330/485 [01:08<00:29,  5.19it/s, now=None]\u001b[A\n",
      "t:  68%|██████▊   | 331/485 [01:08<00:28,  5.44it/s, now=None]\u001b[A\n",
      "t:  68%|██████▊   | 332/485 [01:08<00:28,  5.31it/s, now=None]\u001b[A\n",
      "t:  69%|██████▊   | 333/485 [01:08<00:33,  4.60it/s, now=None]\u001b[A\n",
      "t:  69%|██████▉   | 334/485 [01:09<00:33,  4.53it/s, now=None]\u001b[A\n",
      "t:  69%|██████▉   | 335/485 [01:09<00:30,  4.99it/s, now=None]\u001b[A\n",
      "t:  69%|██████▉   | 336/485 [01:09<00:27,  5.47it/s, now=None]\u001b[A\n",
      "t:  69%|██████▉   | 337/485 [01:09<00:26,  5.69it/s, now=None]\u001b[A\n",
      "t:  70%|██████▉   | 338/485 [01:09<00:24,  5.91it/s, now=None]\u001b[A\n",
      "t:  70%|██████▉   | 339/485 [01:09<00:23,  6.16it/s, now=None]\u001b[A\n",
      "t:  70%|███████   | 340/485 [01:09<00:22,  6.43it/s, now=None]\u001b[A\n",
      "t:  70%|███████   | 341/485 [01:10<00:22,  6.42it/s, now=None]\u001b[A\n",
      "t:  71%|███████   | 342/485 [01:10<00:22,  6.29it/s, now=None]\u001b[A\n",
      "t:  71%|███████   | 343/485 [01:10<00:22,  6.43it/s, now=None]\u001b[A\n",
      "t:  71%|███████   | 344/485 [01:10<00:20,  6.72it/s, now=None]\u001b[A\n",
      "t:  71%|███████   | 345/485 [01:10<00:21,  6.61it/s, now=None]\u001b[A\n",
      "t:  71%|███████▏  | 346/485 [01:10<00:20,  6.64it/s, now=None]\u001b[A\n",
      "t:  72%|███████▏  | 347/485 [01:10<00:21,  6.56it/s, now=None]\u001b[A\n",
      "t:  72%|███████▏  | 348/485 [01:11<00:20,  6.80it/s, now=None]\u001b[A\n",
      "t:  72%|███████▏  | 349/485 [01:11<00:21,  6.44it/s, now=None]\u001b[A\n",
      "t:  72%|███████▏  | 350/485 [01:11<00:20,  6.48it/s, now=None]\u001b[A\n",
      "t:  72%|███████▏  | 351/485 [01:11<00:20,  6.65it/s, now=None]\u001b[A\n",
      "t:  73%|███████▎  | 352/485 [01:11<00:21,  6.07it/s, now=None]\u001b[A\n",
      "t:  73%|███████▎  | 353/485 [01:11<00:21,  6.22it/s, now=None]\u001b[A\n",
      "t:  73%|███████▎  | 354/485 [01:12<00:20,  6.44it/s, now=None]\u001b[A\n",
      "t:  73%|███████▎  | 355/485 [01:12<00:19,  6.70it/s, now=None]\u001b[A\n",
      "t:  73%|███████▎  | 356/485 [01:12<00:18,  6.91it/s, now=None]\u001b[A\n",
      "t:  74%|███████▎  | 357/485 [01:12<00:19,  6.49it/s, now=None]\u001b[A\n",
      "t:  74%|███████▍  | 358/485 [01:12<00:19,  6.58it/s, now=None]\u001b[A\n",
      "t:  74%|███████▍  | 359/485 [01:12<00:19,  6.61it/s, now=None]\u001b[A\n",
      "t:  74%|███████▍  | 360/485 [01:12<00:18,  6.64it/s, now=None]\u001b[A\n",
      "t:  74%|███████▍  | 361/485 [01:13<00:19,  6.41it/s, now=None]\u001b[A\n",
      "t:  75%|███████▍  | 362/485 [01:13<00:21,  5.69it/s, now=None]\u001b[A\n",
      "t:  75%|███████▍  | 363/485 [01:13<00:21,  5.79it/s, now=None]\u001b[A\n",
      "t:  75%|███████▌  | 364/485 [01:13<00:20,  5.92it/s, now=None]\u001b[A\n",
      "t:  75%|███████▌  | 365/485 [01:13<00:20,  5.84it/s, now=None]\u001b[A\n",
      "t:  75%|███████▌  | 366/485 [01:14<00:19,  6.09it/s, now=None]\u001b[A\n",
      "t:  76%|███████▌  | 367/485 [01:14<00:19,  6.05it/s, now=None]\u001b[A\n",
      "t:  76%|███████▌  | 368/485 [01:14<00:18,  6.31it/s, now=None]\u001b[A\n",
      "t:  76%|███████▌  | 369/485 [01:14<00:18,  6.25it/s, now=None]\u001b[A\n",
      "t:  76%|███████▋  | 370/485 [01:14<00:19,  5.76it/s, now=None]\u001b[A\n",
      "t:  76%|███████▋  | 371/485 [01:14<00:18,  6.14it/s, now=None]\u001b[A\n",
      "t:  77%|███████▋  | 372/485 [01:14<00:18,  6.13it/s, now=None]\u001b[A\n",
      "t:  77%|███████▋  | 373/485 [01:15<00:18,  6.09it/s, now=None]\u001b[A\n",
      "t:  77%|███████▋  | 374/485 [01:15<00:17,  6.21it/s, now=None]\u001b[A\n",
      "t:  77%|███████▋  | 375/485 [01:15<00:17,  6.38it/s, now=None]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:  78%|███████▊  | 376/485 [01:15<00:16,  6.57it/s, now=None]\u001b[A\n",
      "t:  78%|███████▊  | 377/485 [01:15<00:17,  6.15it/s, now=None]\u001b[A\n",
      "t:  78%|███████▊  | 378/485 [01:15<00:16,  6.45it/s, now=None]\u001b[A\n",
      "t:  78%|███████▊  | 379/485 [01:16<00:16,  6.46it/s, now=None]\u001b[A\n",
      "t:  78%|███████▊  | 380/485 [01:16<00:18,  5.80it/s, now=None]\u001b[A\n",
      "t:  79%|███████▊  | 381/485 [01:16<00:17,  6.00it/s, now=None]\u001b[A\n",
      "t:  79%|███████▉  | 382/485 [01:16<00:16,  6.22it/s, now=None]\u001b[A\n",
      "t:  79%|███████▉  | 383/485 [01:16<00:15,  6.55it/s, now=None]\u001b[A\n",
      "t:  79%|███████▉  | 384/485 [01:16<00:15,  6.53it/s, now=None]\u001b[A\n",
      "t:  79%|███████▉  | 385/485 [01:17<00:15,  6.66it/s, now=None]\u001b[A\n",
      "t:  80%|███████▉  | 386/485 [01:17<00:15,  6.31it/s, now=None]\u001b[A\n",
      "t:  80%|███████▉  | 387/485 [01:17<00:15,  6.33it/s, now=None]\u001b[A\n",
      "t:  80%|████████  | 388/485 [01:17<00:14,  6.55it/s, now=None]\u001b[A\n",
      "t:  80%|████████  | 389/485 [01:17<00:14,  6.80it/s, now=None]\u001b[A\n",
      "t:  80%|████████  | 390/485 [01:17<00:14,  6.43it/s, now=None]\u001b[A\n",
      "t:  81%|████████  | 391/485 [01:17<00:14,  6.54it/s, now=None]\u001b[A\n",
      "t:  81%|████████  | 392/485 [01:18<00:13,  6.69it/s, now=None]\u001b[A\n",
      "t:  81%|████████  | 393/485 [01:18<00:13,  6.95it/s, now=None]\u001b[A\n",
      "t:  81%|████████  | 394/485 [01:18<00:13,  6.59it/s, now=None]\u001b[A\n",
      "t:  81%|████████▏ | 395/485 [01:18<00:13,  6.72it/s, now=None]\u001b[A\n",
      "t:  82%|████████▏ | 396/485 [01:18<00:13,  6.82it/s, now=None]\u001b[A\n",
      "t:  82%|████████▏ | 397/485 [01:18<00:13,  6.76it/s, now=None]\u001b[A\n",
      "t:  82%|████████▏ | 398/485 [01:18<00:12,  6.95it/s, now=None]\u001b[A\n",
      "t:  82%|████████▏ | 399/485 [01:19<00:12,  6.91it/s, now=None]\u001b[A\n",
      "t:  82%|████████▏ | 400/485 [01:19<00:12,  7.01it/s, now=None]\u001b[A\n",
      "t:  83%|████████▎ | 401/485 [01:19<00:11,  7.16it/s, now=None]\u001b[A\n",
      "t:  83%|████████▎ | 402/485 [01:19<00:11,  7.05it/s, now=None]\u001b[A\n",
      "t:  83%|████████▎ | 403/485 [01:19<00:11,  6.93it/s, now=None]\u001b[A\n",
      "t:  83%|████████▎ | 404/485 [01:19<00:11,  7.02it/s, now=None]\u001b[A\n",
      "t:  84%|████████▎ | 405/485 [01:19<00:11,  7.20it/s, now=None]\u001b[A\n",
      "t:  84%|████████▎ | 406/485 [01:20<00:10,  7.30it/s, now=None]\u001b[A\n",
      "t:  84%|████████▍ | 407/485 [01:20<00:11,  6.74it/s, now=None]\u001b[A\n",
      "t:  84%|████████▍ | 408/485 [01:20<00:11,  6.80it/s, now=None]\u001b[A\n",
      "t:  84%|████████▍ | 409/485 [01:20<00:10,  6.92it/s, now=None]\u001b[A\n",
      "t:  85%|████████▍ | 410/485 [01:20<00:10,  6.98it/s, now=None]\u001b[A\n",
      "t:  85%|████████▍ | 411/485 [01:20<00:11,  6.34it/s, now=None]\u001b[A\n",
      "t:  85%|████████▍ | 412/485 [01:21<00:11,  6.19it/s, now=None]\u001b[A\n",
      "t:  85%|████████▌ | 413/485 [01:21<00:11,  6.44it/s, now=None]\u001b[A\n",
      "t:  85%|████████▌ | 414/485 [01:21<00:10,  6.70it/s, now=None]\u001b[A\n",
      "t:  86%|████████▌ | 415/485 [01:21<00:10,  6.48it/s, now=None]\u001b[A\n",
      "t:  86%|████████▌ | 416/485 [01:21<00:10,  6.48it/s, now=None]\u001b[A\n",
      "t:  86%|████████▌ | 417/485 [01:21<00:10,  6.30it/s, now=None]\u001b[A\n",
      "t:  86%|████████▌ | 418/485 [01:21<00:10,  6.25it/s, now=None]\u001b[A\n",
      "t:  86%|████████▋ | 419/485 [01:22<00:10,  6.05it/s, now=None]\u001b[A\n",
      "t:  87%|████████▋ | 420/485 [01:22<00:10,  6.13it/s, now=None]\u001b[A\n",
      "t:  87%|████████▋ | 421/485 [01:22<00:10,  6.32it/s, now=None]\u001b[A\n",
      "t:  87%|████████▋ | 422/485 [01:22<00:10,  6.25it/s, now=None]\u001b[A\n",
      "t:  87%|████████▋ | 423/485 [01:22<00:09,  6.31it/s, now=None]\u001b[A\n",
      "t:  87%|████████▋ | 424/485 [01:22<00:09,  6.39it/s, now=None]\u001b[A\n",
      "t:  88%|████████▊ | 425/485 [01:23<00:09,  6.58it/s, now=None]\u001b[A\n",
      "t:  88%|████████▊ | 426/485 [01:23<00:08,  6.72it/s, now=None]\u001b[A\n",
      "t:  88%|████████▊ | 427/485 [01:23<00:09,  6.29it/s, now=None]\u001b[A\n",
      "t:  88%|████████▊ | 428/485 [01:23<00:08,  6.44it/s, now=None]\u001b[A\n",
      "t:  88%|████████▊ | 429/485 [01:23<00:08,  6.72it/s, now=None]\u001b[A\n",
      "t:  89%|████████▊ | 430/485 [01:23<00:07,  6.90it/s, now=None]\u001b[A\n",
      "t:  89%|████████▉ | 431/485 [01:23<00:08,  6.40it/s, now=None]\u001b[A\n",
      "t:  89%|████████▉ | 432/485 [01:24<00:08,  6.08it/s, now=None]\u001b[A\n",
      "t:  89%|████████▉ | 433/485 [01:24<00:08,  6.37it/s, now=None]\u001b[A\n",
      "t:  89%|████████▉ | 434/485 [01:24<00:07,  6.42it/s, now=None]\u001b[A\n",
      "t:  90%|████████▉ | 435/485 [01:24<00:07,  6.34it/s, now=None]\u001b[A\n",
      "t:  90%|████████▉ | 436/485 [01:24<00:07,  6.47it/s, now=None]\u001b[A\n",
      "t:  90%|█████████ | 437/485 [01:24<00:07,  6.38it/s, now=None]\u001b[A\n",
      "t:  90%|█████████ | 438/485 [01:25<00:07,  6.53it/s, now=None]\u001b[A\n",
      "t:  91%|█████████ | 439/485 [01:25<00:07,  6.42it/s, now=None]\u001b[A\n",
      "t:  91%|█████████ | 440/485 [01:25<00:06,  6.54it/s, now=None]\u001b[A\n",
      "t:  91%|█████████ | 441/485 [01:25<00:06,  6.79it/s, now=None]\u001b[A\n",
      "t:  91%|█████████ | 442/485 [01:25<00:06,  6.71it/s, now=None]\u001b[A\n",
      "t:  91%|█████████▏| 443/485 [01:25<00:06,  6.69it/s, now=None]\u001b[A\n",
      "t:  92%|█████████▏| 444/485 [01:25<00:06,  6.67it/s, now=None]\u001b[A\n",
      "t:  92%|█████████▏| 445/485 [01:26<00:06,  6.65it/s, now=None]\u001b[A\n",
      "t:  92%|█████████▏| 446/485 [01:26<00:05,  6.83it/s, now=None]\u001b[A\n",
      "t:  92%|█████████▏| 447/485 [01:26<00:05,  6.40it/s, now=None]\u001b[A\n",
      "t:  92%|█████████▏| 448/485 [01:26<00:05,  6.49it/s, now=None]\u001b[A\n",
      "t:  93%|█████████▎| 449/485 [01:26<00:05,  6.64it/s, now=None]\u001b[A\n",
      "t:  93%|█████████▎| 450/485 [01:26<00:05,  6.76it/s, now=None]\u001b[A\n",
      "t:  93%|█████████▎| 451/485 [01:27<00:05,  6.63it/s, now=None]\u001b[A\n",
      "t:  93%|█████████▎| 452/485 [01:27<00:05,  6.43it/s, now=None]\u001b[A\n",
      "t:  93%|█████████▎| 453/485 [01:27<00:04,  6.49it/s, now=None]\u001b[A\n",
      "t:  94%|█████████▎| 454/485 [01:27<00:04,  6.75it/s, now=None]\u001b[A\n",
      "t:  94%|█████████▍| 455/485 [01:27<00:04,  6.36it/s, now=None]\u001b[A\n",
      "t:  94%|█████████▍| 456/485 [01:27<00:04,  6.33it/s, now=None]\u001b[A\n",
      "t:  94%|█████████▍| 457/485 [01:27<00:04,  6.27it/s, now=None]\u001b[A\n",
      "t:  94%|█████████▍| 458/485 [01:28<00:04,  6.46it/s, now=None]\u001b[A\n",
      "t:  95%|█████████▍| 459/485 [01:28<00:04,  6.32it/s, now=None]\u001b[A\n",
      "t:  95%|█████████▍| 460/485 [01:28<00:03,  6.43it/s, now=None]\u001b[A\n",
      "t:  95%|█████████▌| 461/485 [01:28<00:03,  6.67it/s, now=None]\u001b[A\n",
      "t:  95%|█████████▌| 462/485 [01:28<00:03,  6.35it/s, now=None]\u001b[A\n",
      "t:  95%|█████████▌| 463/485 [01:28<00:03,  6.47it/s, now=None]\u001b[A\n",
      "t:  96%|█████████▌| 464/485 [01:29<00:03,  6.67it/s, now=None]\u001b[A\n",
      "t:  96%|█████████▌| 465/485 [01:29<00:02,  6.87it/s, now=None]\u001b[A\n",
      "t:  96%|█████████▌| 466/485 [01:29<00:02,  6.65it/s, now=None]\u001b[A\n",
      "t:  96%|█████████▋| 467/485 [01:29<00:02,  6.72it/s, now=None]\u001b[A\n",
      "t:  96%|█████████▋| 468/485 [01:29<00:02,  6.90it/s, now=None]\u001b[A\n",
      "t:  97%|█████████▋| 469/485 [01:29<00:02,  6.90it/s, now=None]\u001b[A\n",
      "t:  97%|█████████▋| 470/485 [01:29<00:02,  6.87it/s, now=None]\u001b[A\n",
      "t:  97%|█████████▋| 471/485 [01:30<00:02,  6.38it/s, now=None]\u001b[A\n",
      "t:  97%|█████████▋| 472/485 [01:30<00:02,  5.95it/s, now=None]\u001b[A\n",
      "t:  98%|█████████▊| 473/485 [01:30<00:02,  5.77it/s, now=None]\u001b[A\n",
      "t:  98%|█████████▊| 474/485 [01:30<00:01,  5.75it/s, now=None]\u001b[A\n",
      "t:  98%|█████████▊| 475/485 [01:30<00:01,  5.98it/s, now=None]\u001b[A\n",
      "t:  98%|█████████▊| 476/485 [01:30<00:01,  6.19it/s, now=None]\u001b[A\n",
      "t:  98%|█████████▊| 477/485 [01:31<00:01,  5.66it/s, now=None]\u001b[A\n",
      "t:  99%|█████████▊| 478/485 [01:31<00:01,  5.25it/s, now=None]\u001b[A\n",
      "t:  99%|█████████▉| 479/485 [01:31<00:01,  4.81it/s, now=None]\u001b[A\n",
      "t:  99%|█████████▉| 480/485 [01:32<00:01,  3.31it/s, now=None]\u001b[A\n",
      "t:  99%|█████████▉| 481/485 [01:32<00:01,  3.55it/s, now=None]\u001b[A\n",
      "t:  99%|█████████▉| 482/485 [01:32<00:00,  3.52it/s, now=None]\u001b[A\n",
      "t: 100%|█████████▉| 483/485 [01:33<00:00,  3.14it/s, now=None]\u001b[A\n",
      "t: 100%|█████████▉| 484/485 [01:33<00:00,  3.20it/s, now=None]\u001b[A\n",
      "t: 100%|██████████| 485/485 [01:33<00:00,  3.33it/s, now=None]\u001b[A\n",
      "t:  81%|████████  | 391/485 [06:27<00:13,  6.75it/s, now=None]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready challenge_video_processed.mp4\n",
      "CPU times: user 1min 54s, sys: 19.1 s, total: 2min 13s\n",
      "Wall time: 1min 36s\n"
     ]
    }
   ],
   "source": [
    "#define buffer size for smoothing\n",
    "max_queue_size = 5\n",
    "LinesDeque = collections.deque(maxlen=max_queue_size)\n",
    "def process_challenging_video(subclip=False,begin=0,end=5):\n",
    "\n",
    "    output='challenge_video_processed.mp4'\n",
    "    clip1 = VideoFileClip('challenge_video.mp4')\n",
    "    if (subclip):\n",
    "        clip = clip1.fl_image(process_frame).subclip(begin,end)\n",
    "    else:\n",
    "        clip = clip1.fl_image(process_frame)\n",
    "    %time clip.write_videofile(output, audio=False)\n",
    "\n",
    "process_challenging_video(subclip=False,begin=0,end=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src='challenge_video_processed.mp4'>\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process harder challenging video\n",
    "please uncomment the last line to process the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define buffer size for smoothing\n",
    "max_queue_size = 5\n",
    "LinesDeque = collections.deque(maxlen=max_queue_size)\n",
    "def process_challenging_video(subclip=False,begin=0,end=5):\n",
    "\n",
    "    output='hard_challenge_video_processed.mp4'\n",
    "    clip1 = VideoFileClip('harder_challenge_video.mp4')\n",
    "    if (subclip):\n",
    "        clip = clip1.fl_image(process_frame).subclip(begin,end)\n",
    "    else:\n",
    "        clip = clip1.fl_image(process_frame)\n",
    "    %time clip.write_videofile(output, audio=False)\n",
    "\n",
    "process_challenging_video(subclip=False,begin=0,end=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src='hard_challenge_video_processed.mp4'>\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
